<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 5 Multivariada | Metodologia Gauss</title>
  <meta name="description" content="Capítulo 5 Multivariada | Metodologia Gauss" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 5 Multivariada | Metodologia Gauss" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 5 Multivariada | Metodologia Gauss" />
  
  
  

<meta name="author" content="Empresa Junior Gauss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regressão.html"/>
<link rel="next" href="amostragem.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Metodologia</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Bem-vindo</a></li>
<li class="chapter" data-level="" data-path="análise-descritiva.html"><a href="análise-descritiva.html"><i class="fa fa-check"></i>Análise Descritiva</a>
<ul>
<li class="chapter" data-level="1.1" data-path="análise-descritiva.html"><a href="análise-descritiva.html#medidas-de-posição"><i class="fa fa-check"></i><b>1.1</b> Medidas de Posição</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="análise-descritiva.html"><a href="análise-descritiva.html#média"><i class="fa fa-check"></i><b>1.1.1</b> Média</a></li>
<li class="chapter" data-level="1.1.2" data-path="análise-descritiva.html"><a href="análise-descritiva.html#mediana"><i class="fa fa-check"></i><b>1.1.2</b> Mediana</a></li>
<li class="chapter" data-level="1.1.3" data-path="análise-descritiva.html"><a href="análise-descritiva.html#moda"><i class="fa fa-check"></i><b>1.1.3</b> Moda</a></li>
<li class="chapter" data-level="1.1.4" data-path="análise-descritiva.html"><a href="análise-descritiva.html#quartil"><i class="fa fa-check"></i><b>1.1.4</b> Quartil</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="análise-descritiva.html"><a href="análise-descritiva.html#medidas-de-dispersão"><i class="fa fa-check"></i><b>1.2</b> Medidas de Dispersão</a></li>
<li class="chapter" data-level="1.3" data-path="análise-descritiva.html"><a href="análise-descritiva.html#tabela-de-contingência"><i class="fa fa-check"></i><b>1.3</b> Tabela de contingência</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="análise-descritiva.html"><a href="análise-descritiva.html#teste-de-mcnemar"><i class="fa fa-check"></i><b>1.3.1</b> Teste de McNemar</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="análise-descritiva.html"><a href="análise-descritiva.html#tabela-de-classes"><i class="fa fa-check"></i><b>1.4</b> Tabela de classes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="análise-de-correlação.html"><a href="análise-de-correlação.html"><i class="fa fa-check"></i>Análise de Correlação</a>
<ul>
<li class="chapter" data-level="1.5" data-path="análise-de-correlação.html"><a href="análise-de-correlação.html#coeficiente-de-cramér"><i class="fa fa-check"></i><b>1.5</b> Coeficiente de Cramér</a></li>
<li class="chapter" data-level="1.6" data-path="análise-de-correlação.html"><a href="análise-de-correlação.html#coeficiente-de-correlação-ponto-biserial"><i class="fa fa-check"></i><b>1.6</b> Coeficiente de Correlação Ponto Biserial</a></li>
<li class="chapter" data-level="1.7" data-path="análise-de-correlação.html"><a href="análise-de-correlação.html#correlação-linear-de-pearson"><i class="fa fa-check"></i><b>1.7</b> Correlação linear de Pearson</a></li>
<li class="chapter" data-level="1.8" data-path="análise-de-correlação.html"><a href="análise-de-correlação.html#coeficiente-de-correlação-de-spearman"><i class="fa fa-check"></i><b>1.8</b> Coeficiente de correlação de Spearman</a></li>
<li class="chapter" data-level="1.9" data-path="análise-de-correlação.html"><a href="análise-de-correlação.html#coeficiente-tau-de-kendall"><i class="fa fa-check"></i><b>1.9</b> Coeficiente <span class="math inline">\(\tau\)</span> de Kendall</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inferência.html"><a href="inferência.html"><i class="fa fa-check"></i><b>2</b> Inferência</a>
<ul>
<li class="chapter" data-level="2.1" data-path="inferência.html"><a href="inferência.html#teste-de-hipóteses"><i class="fa fa-check"></i><b>2.1</b> Teste de Hipóteses</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="inferência.html"><a href="inferência.html#definição"><i class="fa fa-check"></i><b>2.1.1</b> Definição</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="inferência.html"><a href="inferência.html#testes-de-independência"><i class="fa fa-check"></i><b>2.2</b> Testes de Independência</a></li>
<li class="chapter" data-level="2.3" data-path="inferência.html"><a href="inferência.html#teste-t-para-amostras-independentes"><i class="fa fa-check"></i><b>2.3</b> Teste T para amostras independentes</a></li>
<li class="chapter" data-level="2.4" data-path="inferência.html"><a href="inferência.html#teste-exato-de-fisher"><i class="fa fa-check"></i><b>2.4</b> Teste Exato de Fisher</a></li>
<li class="chapter" data-level="2.5" data-path="inferência.html"><a href="inferência.html#teste-de-bonferroni"><i class="fa fa-check"></i><b>2.5</b> Teste de Bonferroni</a></li>
<li class="chapter" data-level="2.6" data-path="inferência.html"><a href="inferência.html#teste-de-proporção"><i class="fa fa-check"></i><b>2.6</b> Teste de Proporção</a></li>
<li class="chapter" data-level="2.7" data-path="inferência.html"><a href="inferência.html#teste-de-post-hoc"><i class="fa fa-check"></i><b>2.7</b> Teste de Post Hoc</a></li>
<li class="chapter" data-level="2.8" data-path="inferência.html"><a href="inferência.html#teste-de-mcnemar-1"><i class="fa fa-check"></i><b>2.8</b> Teste de McNemar</a></li>
<li class="chapter" data-level="2.9" data-path="inferência.html"><a href="inferência.html#teste-de-wilcoxon-pareado"><i class="fa fa-check"></i><b>2.9</b> Teste de Wilcoxon pareado</a></li>
<li class="chapter" data-level="2.10" data-path="inferência.html"><a href="inferência.html#teste-qui-quadrado"><i class="fa fa-check"></i><b>2.10</b> Teste Qui-Quadrado</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="inferência.html"><a href="inferência.html#teste-qui-quadrado-de-pearson-para-independência"><i class="fa fa-check"></i><b>2.10.1</b> Teste Qui-Quadrado de Pearson para independência</a></li>
<li class="chapter" data-level="2.10.2" data-path="inferência.html"><a href="inferência.html#teste-qui-quadrado-1"><i class="fa fa-check"></i><b>2.10.2</b> Teste Qui-Quadrado</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="estatística-não-paramétrica.html"><a href="estatística-não-paramétrica.html"><i class="fa fa-check"></i><b>3</b> Estatística Não-Paramétrica</a>
<ul>
<li class="chapter" data-level="3.1" data-path="estatística-não-paramétrica.html"><a href="estatística-não-paramétrica.html#teste-de-wilcoxon-mann-whitney"><i class="fa fa-check"></i><b>3.1</b> Teste de Wilcoxon-Mann-Whitney</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="estatística-não-paramétrica.html"><a href="estatística-não-paramétrica.html#definição-1"><i class="fa fa-check"></i><b>3.1.1</b> Definição 1</a></li>
<li class="chapter" data-level="3.1.2" data-path="estatística-não-paramétrica.html"><a href="estatística-não-paramétrica.html#definição-2"><i class="fa fa-check"></i><b>3.1.2</b> Definição 2 :</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regressão.html"><a href="regressão.html"><i class="fa fa-check"></i><b>4</b> Regressão</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regressão.html"><a href="regressão.html#modelos-de-regressão"><i class="fa fa-check"></i><b>4.1</b> Modelos de Regressão</a></li>
<li class="chapter" data-level="4.2" data-path="regressão.html"><a href="regressão.html#coeficiente-de-determinação"><i class="fa fa-check"></i><b>4.2</b> Coeficiente de Determinação</a></li>
<li class="chapter" data-level="4.3" data-path="regressão.html"><a href="regressão.html#transformação-de-yeo-johnson"><i class="fa fa-check"></i><b>4.3</b> Transformação de Yeo-Johnson</a></li>
<li class="chapter" data-level="4.4" data-path="regressão.html"><a href="regressão.html#análise-de-regressão-linear-e-correlação"><i class="fa fa-check"></i><b>4.4</b> Análise de Regressão Linear e Correlação</a></li>
<li class="chapter" data-level="4.5" data-path="regressão.html"><a href="regressão.html#análise-de-variância-multivariada"><i class="fa fa-check"></i><b>4.5</b> Análise de Variância Multivariada</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="regressão.html"><a href="regressão.html#anova-análise-de-variância"><i class="fa fa-check"></i><b>4.5.1</b> (ANOVA) Análise de variância</a></li>
<li class="chapter" data-level="4.5.2" data-path="regressão.html"><a href="regressão.html#manova"><i class="fa fa-check"></i><b>4.5.2</b> MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="regressão.html"><a href="regressão.html#regressão-linear-múltipla"><i class="fa fa-check"></i><b>4.6</b> Regressão Linear Múltipla</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multivariada.html"><a href="multivariada.html"><i class="fa fa-check"></i><b>5</b> Multivariada</a>
<ul>
<li class="chapter" data-level="5.1" data-path="multivariada.html"><a href="multivariada.html#redes-neurais-artificiais"><i class="fa fa-check"></i><b>5.1</b> Redes Neurais Artificiais</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="multivariada.html"><a href="multivariada.html#modelo-mcp"><i class="fa fa-check"></i><b>5.1.1</b> Modelo MCP</a></li>
<li class="chapter" data-level="5.1.2" data-path="multivariada.html"><a href="multivariada.html#funções-de-ativação"><i class="fa fa-check"></i><b>5.1.2</b> Funções de ativação</a></li>
<li class="chapter" data-level="5.1.3" data-path="multivariada.html"><a href="multivariada.html#erro-e-atualização-dos-pesos-no-mcp"><i class="fa fa-check"></i><b>5.1.3</b> Erro e Atualização dos pesos no MCP</a></li>
<li class="chapter" data-level="5.1.4" data-path="multivariada.html"><a href="multivariada.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>5.1.4</b> Multilayer Perceptron (MLP)</a></li>
<li class="chapter" data-level="5.1.5" data-path="multivariada.html"><a href="multivariada.html#algoritmo-back-propagation"><i class="fa fa-check"></i><b>5.1.5</b> Algoritmo back-propagation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multivariada.html"><a href="multivariada.html#análise-de-componentes-principais"><i class="fa fa-check"></i><b>5.2</b> Análise de Componentes Principais</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="amostragem.html"><a href="amostragem.html"><i class="fa fa-check"></i><b>6</b> Amostragem</a></li>
<li class="chapter" data-level="7" data-path="referências.html"><a href="referências.html"><i class="fa fa-check"></i><b>7</b> Referências</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Metodologia Gauss</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariada" class="section level1" number="5">
<h1><span class="header-section-number">Capítulo 5</span> Multivariada</h1>
<div id="redes-neurais-artificiais" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Redes Neurais Artificiais</h2>
<p>Redes neurais artificiais é um método baseado na estrutura de funcionamento dos neurônios do cérebro humano. A arquitetura do método é semelhante a todo o processo que ocorre com os neurônios, quanto a forma de aprendizado e repasse de informação entre neurônios através dos dendritos. A estrutura básica de um neurônio biológico é composto por três seções: o <em>corpo da célula</em>, os <em>dendritos</em> e o <em>axônio</em>. A figura 2.1 é uma ilustração da estrutura de um neurônio biológico</p>
<p><img src="graficos/estrutura-neuronio.jpg" align="center" style="width:70.0%;height:12.0%" /></p>
<p>A partir de inúmeros estudos na área, um modelo em particular baseado em neurônio biológico foi proposto por Warren S. McCulloch e Walter Pitts no ano de 1943 em um artigo
chamado: <em>“A logical calculus of the ideas immanent in nervous activit”</em>, que é uma simplificação do que se sabia até então sobre o neurônio biológico, este modelo foi chamado de MCP.</p>
<div id="modelo-mcp" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Modelo MCP</h3>
<p>A representação matemática do modelo MCP contém <span class="math inline">\(n\)</span> terminais <span class="math inline">\(x_1, x_2, ..., x_n\)</span> (representando os dendritos) e um <span class="math inline">\(y\)</span> (axônio) representando o terminal de saída. Para simular o comportamento das sinapses, os terminais de entrada do neurônio tem pesos <span class="math inline">\(W_1, W_2, ..., W_n\)</span> associados cujo os valores podem ser positivos ou negativos.</p>
<p>No modelo MCP a ativação de um neurônio é obtida através de uma função de ativação, que ativa ou não a saída, dependendo do valor da soma ponderada das suas entradas (Braga, 2007). O modelo terá a saída ativada quando</p>
<p><span class="math display">\[
\begin{eqnarray}
    \sum_{i=1}^{n}x_i W_i \geq \theta,
\end{eqnarray}
\]</span>
em que <span class="math inline">\(n\)</span> é o número de entradas, <span class="math inline">\(W_i\)</span> é o peso associado a entrada <span class="math inline">\(x_i\)</span>, e <span class="math inline">\(\theta\)</span> (threshold) é o limiar do neurônio. A figura 2.2 ilustra o processo do modelo MCP de um neurônio artificial</p>
<p><img src="graficos/Figura-2-Modelo-de-um-Neuronio-Artificial-HAYKIN-2001.png" align="center" style="width:70.0%;height:50.0%" /></p>
</div>
<div id="funções-de-ativação" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Funções de ativação</h3>
<p>Após McCulloch e Pitts terem proposto um modelo, derivou-se outros modelos que permitem a saída não apenas zero ou um, mas diferentes valores para y. Esse processamento se dá pelo que chamamos função de ativação (Braga, 2007). Será exemplificado três tipos de função de ativação :</p>
<ol style="list-style-type: decimal">
<li>Função Linear, definida como:</li>
</ol>
<p><span class="math display">\[   
\begin{eqnarray}
    \hat{y} = \alpha x,
\end{eqnarray}
\]</span></p>
<p>no qual <span class="math inline">\(\hat{y}\)</span> é a saída do modelo, <span class="math inline">\(x\)</span> os valores de entrada e <span class="math inline">\(\alpha\)</span> uma constante de linearidade. No qual o máximo que y pode assumir é <span class="math inline">\(\gamma\)</span> e mínimo -<span class="math inline">\(\gamma\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Função Degrau, definida como</li>
</ol>
<p><span class="math display">\[
 \hat{y} = \begin{cases}
  -\gamma \hbox{, se x $\leq$ - $\gamma$} \\
                               \gamma,\hbox{ se x $\geq$  $\gamma$}  \end{cases}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Função Logística, definida como</li>
</ol>
<p><span class="math display">\[
    \begin{eqnarray}
        \hat{y} = \frac{1}{1 + \epsilon^{x/T}},
    \end{eqnarray}
  \]</span>
onde <span class="math inline">\(T\)</span> determina a suavidade da curva, que será utilizada como função de ligação no modelo proposto.</p>
</div>
<div id="erro-e-atualização-dos-pesos-no-mcp" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Erro e Atualização dos pesos no MCP</h3>
<p>Quando se estima um parâmetro, é esperado que o mesmo apresente algum desvio em torno do valor real. Com o objetivo de minimizar a diferença entre eles, o algorítimo de aprendizado do perceptron dispõe de regras que permite a adaptação dos seus pesos de forma
que a rede execute uma determinada tarefa de classificação.</p>
<p>O algoritmo consiste em atualizar os pesos de modo que estejam mais próximos da solução desejada que os anteriores. A atualização do peso é dada por
<span class="math display">\[
\begin{eqnarray}
    W(t+1) = W(t) + \Delta W, 
\end{eqnarray}
\]</span></p>
<p>em que <span class="math inline">\(W(t)\)</span> é o peso no instante <span class="math inline">\(t\)</span> e <span class="math inline">\(\Delta W= \eta x\)</span> o incremento associado ao peso, sendo <span class="math inline">\(\eta\)</span> a taxa de aprendizado.</p>
<p>Vamos denotar <span class="math inline">\(x\)</span> o vetor de entrada, <span class="math inline">\(y\)</span> o vetor da saída desejada, e {x,<span class="math inline">\(y\)</span>} o nodo arbitrário da rede. A atual saída da rede será chamada de <span class="math inline">\(\hat{y}\)</span>, ou seja, uma estimação do valores ideais, mas com um determinado erro <span class="math inline">\(\epsilon\)</span>. Dessa forma podemos denotar o erro como
<span class="math display">\[
\begin{eqnarray}
    \epsilon = y - \hat{y}.
\end{eqnarray}
\]</span></p>
<p>Para o caso perceptron, <span class="math inline">\(y\)</span> <span class="math inline">\(\in\)</span> <span class="math inline">\(\{0,1\}\)</span> e <span class="math inline">\(\hat{y}\)</span> <span class="math inline">\(\in\)</span> {0,1}. Dessa forma existem quatro possíveis soluções para <span class="math inline">\(\epsilon\)</span>. Se <span class="math inline">\(\epsilon = 0\)</span> então encontramos valores estimados ideais. Caso contrario <span class="math inline">\(\epsilon\)</span> pode assumir valores <span class="math inline">\(\{-1,1\}\)</span>. Se <span class="math inline">\(\epsilon =-1\)</span> implica que <span class="math inline">\(y = 0\)</span> e $  = 1$, mas se <span class="math inline">\(\epsilon =1\)</span> então <span class="math inline">\(y = 1\)</span> e <span class="math inline">\(\hat{y} = 0\)</span>. Com o valor de <span class="math inline">\(\epsilon\)</span> podemos ter uma ideia quanto a direção com que os valores estimados estão distante dos valores ideias (Braga, 2007).</p>
<p>Feito isso, sabemos que <span class="math inline">\(\Delta W\)</span>=<span class="math inline">\(\eta x\)</span> e <span class="math inline">\(w(t + 1) = w(t) + \epsilon \eta x\)</span>. Quando <span class="math inline">\(\epsilon = 1\)</span> a equação de atualização dos pesos possui a forma <span class="math inline">\(W(t+1)= w(t) + \eta x\)</span>. Mas para <span class="math inline">\(\epsilon = -1\)</span> temos a equação <span class="math inline">\(W(t+1)= w(t) - \eta x\)</span>.</p>
</div>
<div id="multilayer-perceptron-mlp" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Multilayer Perceptron (MLP)</h3>
<p>O MLP é uma rede neural semelhante ao perceptron, mas com duas ou mais camadas de neurônios, sendo essas camadas ligadas entre si por sinapses com pesos. O aprendizado deste tipo de rede é feito geralmente pelo algoritmo <em>back-propagation</em> (retro-propagação do erro), porém existem outros algoritmos para serem usados com a mesma finalidade. A figura 2.3 ilustra a estrutura do modelo MLP quanto as entradas (inputs), camadas e a resposta obtida</p>
<p><img src="graficos/mlp2.png" align="center" style="width:70.0%;height:50.0%" /></p>
<p>De forma simplificada a estrutura matemática do modelo é semelhante a do modelo perceptron sendo apenas adicionadas mais camadas de neurônios. A ativação via função de ativação é geralmente feita com a função sigmoidal comumente chamada de função logística (Braga, 2007).</p>
</div>
<div id="algoritmo-back-propagation" class="section level3" number="5.1.5">
<h3><span class="header-section-number">5.1.5</span> Algoritmo back-propagation</h3>
<p>O algoritmo <em>back-propagation</em> é um algoritmo supervisionado que utiliza pares como entrada e saída desejada para por meio de um mecanismo de correção de erros atualizar os pesos. O treinamento tem duas fases chamadas de <em>forward</em> e <em>backward</em>, sendo que o algoritmo percorre a rede em um sentido para cada fase. A fase <em>farward</em> define a saida de rede para uma dada entrada inserida no modelo e a fase <em>backward</em> utiliza a saida fornecida e a saida desejada para atualizar os pesos de sua conexões visando a diminuição do erro (Haykin, 2007). A figura 2.4 ilustra o processo do algoritmo</p>
<p><img src="graficos/Three-layer-back-propagation-neural-network.png" align="center" style="width:70.0%;height:40.0%" /></p>
<p>O algoritmo <em>back-propagation</em> é baseado na regra delta, sendo chamado também de regra delta generalizada. Os ajustes dos pesos são feitos utilizando-se o método gradiente. A função de custo a ser minimizada é dada pela soma dos erros quadráticos
<span class="math display">\[
\begin{eqnarray}
    E = \frac{1}{2}\sum_{p}\sum_{i=1}^{k}(y_{i}^{p}-\hat{y}_{i}^{p}),
\end{eqnarray}
\]</span></p>
<p>em que <span class="math inline">\(p\)</span> é o número de padrões, <span class="math inline">\(k\)</span> é o número de unidades de saída, <span class="math inline">\(y_{i}\)</span> é a i-ésima saída desejada e <span class="math inline">\(\hat{y}_{i}\)</span> é a i-ésima saída da rede.</p>
</div>
</div>
<div id="análise-de-componentes-principais" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Análise de Componentes Principais</h2>
<p>A análise de componentes principais é uma técnica estatística multivariada que consiste na diminuição da quantidade de variáveis em componentes que a partir delas podemos explicar uma determinada proporção da variabilidade total dos dados utilizando apenas as componentes que são essencialmente em menor número que as variáveis.</p>
<p>Seja <span class="math inline">\(a_{ik}\)</span> autovetor correspondente a i-ésima componente principal e a uma variável <span class="math inline">\(k\)</span>. Logo, a n-ésima componente principal para uma observação da amostra <span class="math inline">\(j\)</span>, denominada por <span class="math inline">\(c_{n}^{j}\)</span> definida por:
<span class="math display">\[c_{n}^{j}=\sum_{l=1}^{k}a_{nl}x_{l}^{j},\]</span>
onde, <span class="math inline">\(x_{l}^{j}\)</span> representa o valor da l-ésima variável para uma observação j da amostra.</p>
<p>Para o estudo em questão, os bancos de dados observados foram os do ano de 2014 e 2006 fornecidos pela cliente. Podemos representar matricialmente cada um dos bancos de dados na seguinte forma:</p>
<p><span class="math display">\[
\textbf{X}=
\begin{bmatrix}
    x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1n} \\
    x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2n} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    x_{d1} &amp; x_{d2} &amp; x_{d3} &amp; \dots  &amp; x_{dn}
\end{bmatrix}
\]</span></p>
<p>onde, <span class="math inline">\(n\)</span> é o número de variáveis associadas ao estudo sendo as colunas do nosso banco de dados, e <span class="math inline">\(d\)</span> é a quantidade de observações em cada variável sendo o número de linhas do banco de dados. Com base na matriz de correlação fornecida podemos obter o vetor de autovalores e seus respectivos autovetores.</p>
<p>O vetor de autovalores pode ser representado da seguinte forma:</p>
<p><span class="math display">\[
\boldsymbol{\lambda}=
\begin{bmatrix}
    \lambda_{1;1} \\
    \lambda_{2;1}\\
    \vdots \\
    \lambda_{p;1}
\end{bmatrix}
\]</span></p>
<p>Cada autovalor possui um autovetor que é associado a quantidade de variáveis observadas originalmente (número de colunas). Dessa forma, temos <span class="math inline">\(p\)</span> (quantidade de componentes) autovetores cada um com <span class="math inline">\(n\)</span> (quantidade de variáveis) observações, ou seja, <span class="math inline">\(n\)</span> linhas. Os autovetores podem ser representados matricialmente da seguinte forma:</p>
<p><span class="math display">\[
\textbf{A}_1=
\begin{bmatrix}
    a_{1;1} \\
    a_{2;1}\\
    \vdots \\
    a_{n;1}
\end{bmatrix}
\]</span>
<span class="math display">\[
\textbf{A}_2=
\begin{bmatrix}
    a_{1;1} \\
    a_{2;1}\\
    \vdots \\
    a_{n;1}
\end{bmatrix}
\]</span>

<span class="math display">\[
\textbf{A}_{p}=
\begin{bmatrix}
    a_{1;1} \\
    a_{2;1}\\
    \vdots \\
    a_{n;1}
\end{bmatrix}
.\]</span></p>
<p>Conforme definido anteriormente, o cálculo da n-ésima componente principal para uma observação da amostra j, denominada por <span class="math inline">\(c_{n}^{j}\)</span> é definida por:</p>
<p><span class="math display">\[c_{n}^{j}=\sum_{l=1}^{17}a_{nl}x_{l}^{j}.\]</span></p>
<p>Para facilitar o entendimento, a matriz com todas as componentes principais pode ser representada da seguinte forma:</p>
<p><span class="math display">\[
\textbf{C}=
\begin{bmatrix}
    C_{1}^{1} &amp; C_{2}^{1} &amp;  \dots  &amp; C_{p}^{1} \\
    C_{1}^{2} &amp; C_{2}^{2} &amp;  \dots  &amp; C_{p}^{2} \\
    \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\
    C_{1}^{d} &amp; C_{2}^{d} &amp; \dots  &amp; C_{p}^{d}\\
\end{bmatrix}
.\]</span>
O índice de capital social para cada observação <span class="math inline">\(j\)</span>, denominado por <span class="math inline">\(ICS_j\)</span>, é dado por:</p>
<p><span class="math display">\[ICS_j=\frac{1}{\sum_{k=1}^{p}\lambda_k}\sum_{k=1}^{p}\lambda_k c_{k}^{j},\]</span>
em que <span class="math inline">\(\lambda_k\)</span> representa o autovalor associado à k-ésima componente principal, sendo <span class="math inline">\(p\)</span> o número de componentes principais utilizadas na construção do índice (quantidade definida a partir da variabilidade miníma explicada aceitável).</p>
<p>O vetor com todos os índices são dados na forma:</p>
<p><span class="math display">\[
\textbf{I}=
\begin{bmatrix}
    ICS_{1;1} \\
    ICS_{2;1}\\
    \vdots \\
    ICS_{j;1}\\
    \vdots\\
    ICS_{d;1}
\end{bmatrix}
,\]</span></p>
<p>sendo <span class="math inline">\(d\)</span> o número de observações.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regressão.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="amostragem.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/gaussufc/YMetodologia/edit/master/07-multivarida.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/gaussufc/YMetodologia/blob/master/07-multivarida.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
