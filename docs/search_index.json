[["index.html", "Metodologia Gauss Um pouco sobre Estatística Capítulo 1 Bem-vindo", " Metodologia Gauss Um pouco sobre Estatística Empresa Junior Gauss Capítulo 1 Bem-vindo Universidade federal de Fortaleza | Departamento de Estatística e matemática aplicada \\[\\\\[1in]\\] Este é um livro para facilitar a vida dos Gaussianos com as metodologias para qualquer tipo de trabalhos. Para isso foi feito no github com todos as informações desse livro e foi feito site para disponibiliza-lo online de formar rapida e acessivel a todos, cuide bem desse material. "],["análise-descritiva.html", "Análise Descritiva 1.1 Medidas de Posição 1.2 Medidas de Dispersão 1.3 Tabela de contingência 1.4 Tabela de classes", " Análise Descritiva 1.1 Medidas de Posição 1.1.1 Mediana 1.1.2 Moda A moda é a realização mais frequente em um conjunto de valores. O fenômeno acontece quando, em um banco de dados, há a repetição das informações encontradas em uma mesma variável. Uma empresa de Tecnologia da Informação tem 20 funcionários contratados. A maioria deles (15) tem idade entre 20 e 25 anos. O restante (5) está na faixa de 30 e 40 anos. A maior recorrência de idade, no entanto, é de funcionários com 23 anos. São 5 no total  definindo a medida de posição ora apresentada. A moda pode ser definida em bimodal (quando ocorre a repetição de dois valores) ou multimodal (mais de dois valores repetidos). O caso dos cinco funcionários com 23 anos identificamos como multimodal. 1.1.3 Quartil Temos que os Quartis são valores de divisão na qual divide os conjuntado de dados em 4 partes, na qual temos: Primeiro Quartil divide em \\(25\\%\\) em uma amostra ordena os valores inferiores. Segundo Quartil ou Mediana divide em \\(50\\%\\) em uma amostra ordena os valores inferiores. Terceiro Quartil divide em \\(75\\%\\) em uma amostra ordena os valores superiores. São utilizados para entender como se comporta os dados em cada quartil correspondente até aquele ponto. Intervalo interquartil(IIQ) avalia a dispersão de dados somente depois de ordená-los em ordem crescente. O intervalo interquartil é calculado com base no cálculo de quartis, sendo o primeiro quartil (inferior), o quartil intermediário (mediana), o terceiro quartil (superior), que estão ligados ao conceito de quantil. A diferença entre o quartil superior e o quartil inferior determina o intervalo interquartil 1.2 Medidas de Dispersão 1.3 Tabela de contingência As tabelas de contingência são usadas para registrar observações independentes de duas ou mais variáveis aleatórias, normalmente qualitativas. Suponha que tenhamos duas variáveis de uma população, A e B, e queremos relacioná-la com outras duas variáveis C e D . Retirando-se uma amostra aleatória dessa população, uma tabela de contingência conteria as frequências em cada classe, a tabela seria da seguinte forma: Variáveis Variáveis C D A 5 3 B 3 49 Total 8 52 Em posse da tabela de contingência podemos então realizar testes para saber se há ou não independência entre variáveis. 1.3.1 Teste de McNemar O teste de McNemar é utilizado em tabelas de contingência, essencialmente em tabelas \\((2\\times2)\\), com dados pareados para comparar frequências marginais, que em outras palavras significa que o teste compara se houve mudança nas proporções no objeto de estudo antes e depois de uma intervenção. As hipóteses do teste são: \\[ \\left\\{ \\begin{array}{l} H_0: \\text{As variáveis linha e coluna possuem as mesmas proporções},\\\\ H_1: \\text{As variáveis linha e coluna possuem proporções diferentes}. \\\\ \\end{array} \\right. \\] A estatística do teste, \\(Q\\), é dada por: \\[ Q=\\cfrac{(a-d)^2}{(a+d)} \\] em que \\(Q \\sim \\chi^2(1)\\), lê-se \\(Q\\) segue uma distribuição qui-quadrado com 1 grau de liberdade. 1.4 Tabela de classes A tabela de classes é usada quando temos dados brutos provenientes de uma variável contínua, e então nós as agrupamos para a construção de uma tabela, em intervalos que também são conhecidos por classes. Suponhamos que desejamos construir K classes. O valor mínimo da nossa amostra(mín) e o máximo(máx). A partir dessas informações, calculamos a amplitude total(AT): \\[ \\text{AT = máx  mín.} \\] Como o número de classes (k) é dada por: \\[ k = 1+3,3$\\log(n)$. \\] A amplitude de cada classe (h) é dada por: \\[ h = \\frac{AT}{k}. \\] "],["análise-de-correlação.html", "Análise de Correlação 1.5 Coeficiente de Cramér 1.6 Coeficiente de Correlação Ponto Biserial 1.7 Correlação linear de Pearson 1.8 Coeficiente de correlação de Spearman 1.9 Coeficiente \\(\\tau\\) de Kendall", " Análise de Correlação 1.5 Coeficiente de Cramér O coeficiente de Cramér é uma medida entre duas variáveis medidas numa escala categórica. Portanto pode ser aplicado em situações onde a informação se encontra distribuída por categorias nominais não ordenáveis. Este coeficiente obtém-se diretamente a partir da estatística \\(\\chi_v^2\\) através da expressão \\[ C = \\sqrt{\\dfrac{\\chi_v^2}{n(l-1)}} \\] onde n representa o número total de observações e l representa o mínimo entre o número de linhas e colunas na tabela de contingência. A partir do valor do coeficiente de Cramér é possível efetuar um teste às hipóteses \\[ \\begin{cases} H_0:\\text{ As variáveis são independentes;} \\\\ H_1:\\text{ As variáveis não são independentes.} \\end{cases} \\] 1.6 Coeficiente de Correlação Ponto Biserial Esse coeficiente (\\(r_{pb}\\)) mede a correlação entre uma variável quantitativa e uma variável categórica binária (0,1). Ele pode variar entre -1 e 1 e é definido pela seguinte fórmula: \\[ r_{pb} = \\dfrac{M_1-M_0}{S_n}\\sqrt{pq} \\] \\(M_1\\) = média do grupo que recebeu o valor 1 da variável binária. \\(M_0\\) = média do grupo que recebeu o valor 0 da variável binária. \\(S_n\\) = desvio padrão da variável binária toda. \\(p\\) = proporção de casos do grupo 0 na variável binária. \\(q\\) = proporção de casos do grupo 1 na variável binária. 1.7 Correlação linear de Pearson De modo geral, a quantificação do grau de associação entre duas variáveis é feita pelos chamados coeficientes de associação ou correlação. Essas são medidas que descrevem numericamente a associação (ou dependência) entre duas variáveis. Ou seja, tais resultados nos permitem verificar se uma variável influencia de forma positiva ou negativa na outra. Para facilitar a compreensão, esses coeficientes usualmente variam entre -1 e +1, e quanto mais próximo de 0, menos relação as variáveis possuem. A intensidade da associação linear existente entre as variáveis quantitativas pode ser determinada através do chamado Coeficiente de Correlação Linear de Pearson, e sua fórmula é definida por: \\[\\rho_{(x,y)} = \\frac{cov{(x,y)}}{S_{x}S_{y}},\\] em que , \\(\\text{Cov}{(x,y)}\\) - Covariância ou variância conjunta das variáveis X e Y; \\(S_{x}\\)- Desvio Padrão da variável X \\(S_{Y}\\) - Desvio Padrão da variável Y. Como dito anteriormente, o \\(\\rho\\) pode assumir valores entre -1 e +1 e neste ponto cabe algumas observações, tais como \\(\\rho_{(x,y)}\\) = +1 significa uma correlação positiva perfeita entre as duas variáveis, ou seja, se uma aumenta a outra também aumenta; \\(\\rho_{(x,y)}\\) = -1 significa uma correlação negativa perfeita entre as duas variáveis, isto é, se uma aumenta a outra diminui; \\(\\rho_{(x,y)}\\) = 0 significa que as duas variáveis não dependem linearmente uma da outra. No entanto, pode existir uma outra dependência não-linear. Logo, o resultado deve ser investigado por outros meios. 1.8 Coeficiente de correlação de Spearman O coeficiente de Correlação de Spearman é geralmente utilizado quando temos duas variáveis ordinais ou quando não podemos utilizar o teste Cramér V, pois não podemos facilmente atestar suas suposições. Diante de duas variáveis medidas numa escala de ordenação clara, ou que apresentam uma relação não linear mas monótona (se uma aumenta a outra tem sempre tendência a aumentar (ou a diminuir)). É um método não-paramétrico que usa os postos das variáveis quando o coeficiente de correlação de Pearson não pode ser aplicado, temos como alternativa o coeficiente de correlação de Spearman. A ideia de construção deste coeficiente é bastante simples. Dadas duas amostras de observação ordenáveis, substitui-se cada um dos seus valores pela sua ordem de ordenação, em inglês Por exemplo, se uma amostra de três valores for \\(x_{1} = 2.1\\), \\(x_{2} = 1.7\\), \\(x_{3} = 4.8\\), então os respectivos ranks serão \\(r_{1}= 2\\), \\(r_{2} = 1\\), \\(r_{3} = 3\\). Após substituir cada uma das amostras pelos seus ranks o coeficiente de Spearman não e mais do que o coeficiente de Pearson aplicado agora aos . Uma vez que as ordens variam sempre entre 1 e \\(n\\) (número de observações), pode-se escrever o coeficiente do seguinte modo \\[ R_{s}=1- \\dfrac{ 6\\sum_{i=1}^{n} D_{i}^2}{n^{3} - n}, \\] onde \\(D_{i}\\) representa a diferença de ranks entre as observações que estão sendo analisadas. 1.9 Coeficiente \\(\\tau\\) de Kendall Uma alternativa ao coeficiente de Spearman é o coeficiente de correlação \\(\\tau\\) de Kendall que se aplica nas mesmas condições, porém com duas vantagens : se as amostras tiverem dimensão muito reduzida e valores repetidos, os resultados do teste são mais precisos; por outro lado, o coeficiente \\(\\tau\\) de Kendall pode ser generalizado para correlações parciais que são correlações medidas entre duas variáveis após remoção do efeito de uma possível terceira variável sobre ambas. O coeficiente de Kendall é descrito como uma medida de concordância entre dois conjuntos, medindo a diferença entre a probabilidade de as classificações estarem na mesma ordem e a probabilidade de estarem em ordens diferentes. \\[ \\tau = \\dfrac{ concordantes - discordantes}{\\frac{n(n-1)}{2}}. \\] "],["inferência.html", "Capítulo 2 Inferência 2.1 Teste de Hipóteses 2.2 Testes de Independência 2.3 Teste T para amostras independentes 2.4 Teste Exato de Fisher 2.5 Teste de Bonferroni 2.6 Teste de Proporção 2.7 Teste de Post Hoc 2.8 Teste de McNemar 2.9 Teste de Wilcoxon pareado 2.10 Teste Qui-Quadrado", " Capítulo 2 Inferência 2.1 Teste de Hipóteses 2.1.1 Definição Um teste de hipóteses estatístico é um procedimento que utilizamos para testar duas hipóteses disjuntas a respeito de um parâmetro de interesse. Utiliza-se a nomenclatura \\(H_0\\) e \\(H_1\\) para representar as hipóteses, as quais são chamadas de hipótese nula e hipótese alternativa, respectivamente. Quando testamos uma hipótese, podemos encontrar dois tipos de erros: - Erro Tipo I: Consiste em rejeitarmos a hipótese nula, dado que ela é vera dadeira. Geralmente, representa-se por \\(\\alpha\\) a probabilidade de cometer esse tipo de erro. - Erro Tipo II: Consiste em não rejeitarmos \\(H_0\\), dado que ela é falsa. Normalmente representa-se com a letra grega \\(\\beta\\) a probabilidade de cometer esse erro. Ambos os erros devem ser evitados, porém, por questões matemáticas, geralmente só podemos controlar facilmente o Erro Tipo I, por isso, devemos estabelecer as hipóteses de forma que o Erro Tipo I seja o erro mais prejudicial ao processo estudado. Nível de significância: O valor de \\(\\alpha\\), referente ao Erro Tipo I, é chamado de nível de significância, esse valor deve ser pré-estabelecido pelo pesquisador. Estatística de teste: É a estatística amostral na qual basearemos seus valores para decidirmos pela rejeição ou não de \\(H_0\\). Essa estatística é associada ao estimador do parâmetro que se deseja testar. Por exemplo, ao queremos testar algo sobre o real valor da média populacional \\(\\mu\\) (parâmetro),quando conhece-se o valor do desvio padrão populacional (\\(\\sigma\\)) utiliza-se a a estatística de teste abaixo: \\[ Z = \\dfrac{\\bar{X}-\\mu}{\\sigma/\\sqrt n} \\] Onde: \\(\\bar{X}\\) é a média amostral, sendo ela o estimador natural da média populacional; n é o tamanho da nossa amostra. Região de Rejeição: É a região formada pelo conjunto de valores que levam \\(H_0\\) a ser rejeitada. O valor que delimita essa região é denominado valor crítico. Caso o valor calculado da estatística de teste caia nessa região, rejeita-se a hipótese nula. Nível descritivo ou valor-p: É o menor valor para o qual rejeita-se a hipótese nula. Quando esse valor cai na região crítica, rejeita-se \\(H_0\\). Para sabermos se rejeitaremos ou não \\(H_0\\) baseado no valor-p, deve-se compará-lo com o nível de significância do teste e rejeitar \\(H_0\\), se aquele for menor do que este, rejeita-se \\(H_0\\). 2.2 Testes de Independência Como em todo teste de hipóteses, devemos estabelecer primeiro as hipóteses a serem testadas. Num teste de independência essas hipóteses são: H_0: As variáveis de classificação são independentes. H_1: As variáveis de classificação não são independentes. Existem vários testes de independência em que cada um desses são adequados para uma determinada situação 2.3 Teste T para amostras independentes O teste T é utilizado para verificar a existência de diferença entre as médias de dois grupos da mesma população. Para isso a variável independente deve ser necessariamente métrica e a variável dependente deve ser categórica. Suponha que o objetivo seja analisar as médias amostrais \\(\\bar{x}_1\\) e \\(\\bar{x}_2\\) Assim, testa-se a hipótese como segue: \\[\\begin{cases} H_0: \\bar{x}_1 = \\bar{x}_2 \\\\ H_1: \\bar{x}_1 \\neq \\ \\bar{x}_2 \\end{cases}\\] Suponha que o objetivo seja analisar as médias amostrais \\(\\bar{x}_1\\) e \\(\\bar{x}_2\\) \\[t = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{s^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\] Onde o numerador é a diferença das médias amostrais e o denominados é o desvio padrão amostral 2.4 Teste Exato de Fisher O Teste Exato de Fisher é utilizado para verificar se existe associação entre as variáveis linha e as variáveis coluna em uma tabela de contingência construída a partir dos dados da amostra. Embora na prática o Teste Exato de Fisher seja aplicado quando os tamanhos das amostras são pequenos, o teste é válido para todos os tamanhos amostrais. As hipóteses do teste são: \\[ \\begin{cases} H_0: \\text{As variáveis linha e coluna são independentes}; \\\\ H_1: \\text{As variáveis linha e coluna não são independentes.} \\end{cases} \\] A estatística de teste, apresentada por Fisher (1934), propõe que a distribuição de probabilidade das frequências de quaisquer tabelas de contingência sejam substituídas pela probabilidade da distribuição das mesmas frequências. Considerando a Tabela , arranjada de modo que \\(n_{1.}\\leq n_{.1}\\leq n_{.2}\\leq n_{2.}\\), temos uma distribuição de probabilidade hipergeométrica para a única frequência de valor independente, \\(o_{11}\\), a partir de, \\[P[X=o_{11}]=\\cfrac{\\displaystyle{n_{1.} \\choose o_{11}} \\displaystyle{n_{2.} \\choose o_{21}}}{\\displaystyle{n \\choose n_{.1}}}=\\cfrac{n_{1.}!n_{2.}!n_{.1}!n_{.2}!}{o_{11}!o_{12}!o_{21}!o_{22}!n!}\\] O Teste Exato de Fisher consiste na determinação desta probabilidade e dos arranjos possíveis que, com os mesmos totais marginais, tenham ainda mais desvios em relação à hipótese nula \\(H_0\\). 2.5 Teste de Bonferroni O teste de bonferroni consiste na realização de um teste t para cada par de médias a uma taxa de erro por comparação (TPC) de \\(a/(k/2)\\). Usando esse teste, o nível de significância da família é no máximo \\(a\\), para qualquer média das populações. Assim o teste de Bonferroni protege a taxa de erro da família dos testes. Pode ser usado para quaisquer que sejam os dados balanceados ou não. Não é um teste exato, e assim é baseado na aproximação como primeira desigualdade de Bonferroni. Em diversos momentos ele pode ser um teste conservativo, isto é, a taxa de erro da família testes é muito menor que o nível de significância \\(a\\). Para tamanhos de amostras iguais o teste de Bonferroni considera duas médias significativamente diferentes se o valor absoluto das diferenças ultrapassar: \\[ LSD=t_{(a,N-k)}\\sqrt{2\\frac{QME}{n}} \\] E para os dados não balanceados temos: \\[ LSD=t_{(a,N-k)}\\sqrt{QME(\\frac{1}{n_i}+\\frac{1}{n_j})} \\] Em que \\(a^\\prime=\\frac{1}{2}(a/c)\\) e c é o número de comparações duas a duas. O quantil \\(t_{(a,N-k)}\\) é da distribuição t-Student com parâmetros \\(N-k\\). Assim a margem de erro da equação anterior depende do número de comparações. 2.6 Teste de Proporção Temos que testar hipóteses e construir intervalos de confiança para a proporção de uma população. Supondo que uma amostra aleatória de tamanho n tenha sido retirada de uma grande população e que X são as observações, nessa amostra pertencem a uma classe de interesse. Então \\(P=X/n\\) é um estimador da proporção \\(p\\) da população, que pertence a essa classe, \\(n\\) e \\(p\\) são parâmetros da Distribuição Binomial.,em que \\(n\\) é o tamanho amostral e \\(p\\) a proporção. Para o teste bilateral: \\[ \\begin{cases} H_0: p=p_0\\\\ H_1: p \\neq p_0 \\end{cases} \\] A estatística de teste é: \\[Z_0=\\frac{X-np_0}{\\sqrt{nX/n(1-X/n)}}\\] Rejeitamos a hipótese nula se \\[ Z_0&lt;- Z_{1-\\alpha/2} \\,\\text{ ou }\\, Z_0 &gt; Z_{1-\\alpha/2} \\] Para o teste unilateral a direita: \\[ \\begin{cases} H_0: p=p_0\\\\ H_1: p &gt; p_0 \\end{cases} \\] A estatística de teste é: \\[Z_0=\\frac{X-np_0}{\\sqrt{nX/n(1-X/n)}}\\] Rejeitamos a hipótese nula se \\[ Z_0 &gt; Z_{1-\\alpha} \\] 2.7 Teste de Post Hoc Análise Post Hoc é feita após a conclusão do estudo, usando o tamanho do estudo e de efeito obtidos para determinar a potência da amostra estudada, podemos dizer que são conjuntos de teste para determinar as diferenças presentes. Os testes são para comparações pareadas, projetadas afim de comparar todas as diferentes combinações dos grupos de tratamento. Observando as taxas erro, temos: Erro tipo 1: É a probabilidade de rejeitar a hipótese nula (Ho), quando ela é verdadeira. Erro tipo 2: É a probabilidade de não rejeitar a hipótese nula (Ho), quando ela é falsa. Probabilidade de acontecer pelo menos um erro do Tipo I em um conjunto de relações estatísticas, assumindo erroneamente que pelo menos uma das diferenças analisadas é significativamente diferente da hipótese nula. \\[ { \\alpha_{F_W}}= 1-(1- {\\alpha})^c \\] Onde C é o número de comparações, então dessa forma temos que: \\[ c=\\frac{k*(k-1)}{2} \\] 2.8 Teste de McNemar O teste de McNemar é utilizado em tabelas de contingência, essencialmente em tabelas \\((2\\times2)\\), com dados pareados para comparar frequências marginais, que em outras palavras significa que o teste compara se houve mudança nas proporções no objeto de estudo antes e depois de uma intervenção. As hipóteses do teste são: \\[ \\begin{cases} H_0: \\text{As variáveis linha e coluna possuem as mesmas proporções;}\\\\ H_1: \\text{As variáveis linha e coluna possuem proporções diferentes.} \\end{cases} \\] A estatística do teste, \\(Q\\), é dada por: \\[ Q=\\cfrac{(a-d)^2}{(a+d)}\\] em que \\(Q \\sim \\chi^2(1)\\), lê-se \\(Q\\) segue uma distribuição qui-quadrado com 1 grau de liberdade. 2.9 Teste de Wilcoxon pareado O teste de Wilcoxon pareado é utilizado para comparar se dois grupos possuem a mesma medida de tendência central. Esse teste leva em consideração a magnitude das diferenças entre os pares. Seja \\(d_i\\) o escore-diferença para qualquer par combinado, representando a diferença dos pares sobre dois tratamentos \\(X\\) e \\(Y\\), isto é, \\(d_i=X_i-Y_i\\). Ordena-se todos os \\(d_i\\)s sem considerar o sinal, posteriormente dar-se o posto 1 ao menor \\(|d_i|\\), posto 2 ao segundo menor, e assim sucessivamente. As possíveis hipóteses do teste são: \\(H_0\\): \\(d=0\\) vs \\(H_1\\): \\(d \\neq 0\\); \\(H_0\\): \\(d=0\\) vs \\(H_1\\): \\(d&gt;0\\); \\(H_0\\): \\(d=0\\) vs \\(H_1\\): \\(d&lt;0\\). A hipótese nula (\\(H_0\\)) é que os tratamentos \\(X\\) e \\(Y\\) são equivalentes, isto é, eles são amostras de populações com a mesma mediana e a mesma distribuição contínua. Duas estatísticas são definidas: \\[ T^+=\\sum_{i}d_i, \\] \\[ T^-=\\sum_{i}d_i, \\] soma dos \\(d_i\\)s positivos e soma dos \\(d_i\\)s negativos, respectivamente. Rejeitamos a hipótese nula se a probabilidade de \\(T^+\\) tabulado para um determinado tamanho \\(N\\) é menor ou igual ao nível de significância escolhido. 2.10 Teste Qui-Quadrado 2.10.1 Teste Qui-Quadrado de Pearson para independência O teste de independência Qui-Quadrado é usado para descobrir se existe uma associação entre a variável de linha e a variável de coluna em uma tabela de contingência construído à partir de dados da amostra. A hipótese nula é de que as variáveis não estão associadas, em outras palavras, eles são independentes. A hipótese alternativa é de que as variáveis estão associadas, ou dependentes. Para que o teste possa ser aplicado, é necessário levar em consideração algumas suposições e condições, que serão listadas a seguir: Os dados devem ser derivados de contagens(frequências) para as categorias das variáveis categóricas; As frequência das células da tabela de dupla entrada devem ser independentes umas das outras; Os sujeitos contados na tabela devem ser de uma amostra aleatória extraída de uma única população; Devemos ter dados suficientes; A tabela de contingência(dupla entrada) deve conter pelo menos 5 observações em cada célula; A tabela de valores esperados não pode conter observações menores que 5 O teste é aplicável tanto para a variável qualitativa quanto quantitativa. As hipóteses do teste são: \\[ \\begin{cases} H_0: \\text{As variáveis X e Y são independentes};\\\\ H_1: \\text{As variáveis X e Y não são independentes.} \\end{cases} \\] A Estatística do teste é dada por: \\[\\chi^2 = \\sum_i\\sum_j\\frac{(o_{ij} - e_{ij}^2)}{e_{ij}}\\] onde: - \\(o_{ij}\\) é o valor observado na célula; - \\(e_{ij}\\) é o valor esperado - \\(e_{ij} = \\frac{\\text{total da linha i} * \\text{total da coluna j}}{\\text{total geral}}\\) Rejeitamos \\(H_0\\) se \\(\\chi^2_{calculado} &gt; \\chi^2_{tabelado}\\) 2.10.2 Teste Qui-Quadrado O teste \\(\\chi^2\\) ( Qui- Quadrado) é um dos testes usados para avaliar se há independência entre variáveis qualitativas, para isso, devemos dispor nossos dados em uma tabela de contingência, no caso em que queremos testar a hipótese de independência entre exatamente duas variáveis, utiliza-se uma tabela de dupla entrada. Para realizar o teste, iremos testar a hipótese como segue: \\[\\begin{cases} H_0: \\text{As variáveis são independentes;} \\\\ H_1: \\text{As variáveis não são independentes.} \\end{cases} \\] Para isso, utilizaremos a estatística de teste abaixo: \\[ \\mathbf{\\chi^2_v} = \\dfrac{\\sum_{\\substack{i=1}}^{k} \\sum_{\\substack{i=1}}^{l}(O_{ij}-E_{ij})^2}{E_{ij}} \\] onde: k é o número de linhas da tabela ; l é o número de colunas; \\(E_{ij} = n p_{ij}\\) é a frequência esperada da célula ij; \\(p_{ij}\\) é a probabilidade de ocorrer uma observação na célula ij. Se as variáveis são independentes, espera-se que \\(p_{ij} = p_{i.}p_{j.}\\), onde \\(p_i{i.}\\) é a probabilidade marginal correspondente a linha i e \\(p_{.j}\\) é a probabilidade marginal correspondente a coluna j; \\(v = (k-1)(l-1)\\) é o grau de liberdade. Assim como nos demais testes de hipóteses, rejeitaremos \\(H_0\\) caso a probabilidade encontrada seja menor do que o nível de significância estipulado. Porém, esse teste apresenta problemas quando temos um número de observações considerado pequeno - geralmente considera-se pequeno um tamanho amostral menor do que 40 unidades - ou quando temos em uma das caselas de cruzamento um número esperado menor do que 5. Nesses casos, utiliza-se uma correção para uma melhor estimação. A correção indicada para esse caso é a de Yates ou de Continuidade, pela qual o valor do teste Qui-Quadrado será recalculado como se segue: \\[ \\mathbf{\\chi^2_v} = \\dfrac{\\sum_{\\substack{i=1}}^{k} \\sum_{\\substack{i=1}}^{l}(|O_{ij}-E_{ij}|-0,5)^2}{E_{ij}}. \\] "],["estatística-não-paramétrica.html", "Capítulo 3 Estatística Não-Paramétrica 3.1 Teste de Wilcoxon-Mann-Whitney", " Capítulo 3 Estatística Não-Paramétrica 3.1 Teste de Wilcoxon-Mann-Whitney 3.1.1 Definição 1 Proposto inicialmente por Frank Wilcoxon (1945) e com contribuições de Henry B. Mann e Donald R. Whitney (1947), o teste de Wilcoxon-Mann-Whitney é um teste não paramétrico aplicado em duas amostras independentes. Esse teste é baseado nos postos dos valores obtidos combinando-se as duas amostras. Isso é feito ordenando-se esses valores, do menor para o maior, independentemente do fato de qual população cada valor provém, caso haja observações repetidas atribui-se a média dos postos correspondentes. O teste de Wilcoxon-Mann-Whitney tem como objetivo testar se as distribuições possuem a mesma medida de tendência central. Seja \\(X_1\\), \\(X_2\\), , \\(X_m\\) uma amostra aleatória de \\(X\\) de modo que \\(X_j\\)s são independentes e identicamente distribuídos e \\(Y_1\\), \\(Y_2\\), , \\(Y_n\\) uma amostra aleatória de \\(Y\\) de modo que os \\(Y_i\\)s são independentes e identicamente distribuídos. Além disso, suponha que os \\(X_j\\)s e os \\(Y_i\\)s são mutuamente independentes e a amostra \\(Y\\) aquela com o menor tamanho amostral, isto é, \\(n \\leq m\\). As possíveis hipóteses do teste são: \\(H_0\\): \\(\\Delta=0\\) vs \\(H_1\\): \\(\\Delta\\neq0\\); \\(H_0\\): \\(\\Delta=0\\) vs \\(H_1\\): \\(\\Delta&gt;0\\); \\(H_0\\): \\(\\Delta=0\\) vs \\(H_1\\): \\(\\Delta&lt;0\\). em que: \\(\\Delta\\) = Estimador da diferença das posições. Para estimar a diferença \\(\\Delta\\) entre as medianas das populações, consideramos todas as \\(m \\times n\\) diferenças \\(y_i - x_j\\) ordenadas de forma crescente. O estimador \\(\\hat{\\Delta}\\) associado a estatística de Wilcoxon-Mann-Whitney é definido por \\(\\hat{\\Delta}= \\ \\hbox{mediana}\\{(y_i-x_j), \\ i=1,\\ldots, n; j=1,\\ldots,m\\}\\), chamado de pseudo-mediana. A estatística de teste, denotada por \\(W\\), é dada pelo mínimo de \\(U_m\\) e \\(U_n\\) \\[ \\begin{align*} U_m &amp;= S_m - \\frac{m(m+1)}{2} \\\\ U_n &amp;= S_n - \\frac{n(n+1)}{2} \\end{align*} \\] em que: \\(S_m\\) = Soma dos postos relacionados a amostra \\(X\\); \\(S_n\\) = Soma dos postos relacionados a amostra \\(Y\\). 3.1.2 Definição 2 : O teste de Wilcoxon-Mann-whitney é um teste não-paramétrico, ou seja, é um teste usado quando temos poucas ( ou nenhuma) evidências sobre a real distribuição dos dados. Ele se utiliza de passos idênticos aos que usamos para calcular um teste de hipóteses usual. Seja \\(P_1\\) e \\(P_2\\) duas população das quais não temos informações sobre suas distribuições, porém, temos que seus dados são qualitativo ordinais ou quantitativos. Retira-se uma amostra de cada uma dessas populações, sendo essas independentes. O procedimento utilizado nesse teste consiste em ordenarmos essas duas amostras de forma combinada, ou seja, ordena-se esses valores sem a necessidade de levar em consideração de qual população o dado provém. Com isso, testaremos se as duas populações têm a mesma mediana, ou seja, se suas distribuições são iguais em localização. Sendo \\(X_1,X_2,...,X_m\\) e \\(Y_1,Y_2,...,Y_n\\), amostras aleatórias independentes e identicamente distribuídas(i.i.d) das populações \\(P_1\\) e \\(P_2\\), respectivamente. Além disso, essas amostras também serão independentes entre si, sendo essas tomadas de forma que \\(n\\leq m\\). Sendo F e G as funções de distribuição correspondentes as populações \\(P_1\\) e \\(P_2\\), respectivamente, a hipótese nula é tal como segue: \\[ H_0: F(t) = G(t),\\text{para todo t} \\] Entretanto, podemos também considerar que Y tem a mesma distribuição de X+\\(\\Delta\\). Perante isso, a hipótese nula será: \\[ H_0: \\Delta = 0 \\] Daí, como estamos interessados apenas em haver diferença entre as medianas, independentemente se essa diferença é negativa ou positiva, utilizamos como hipótese alternativa, uma hipótese bilateral, como vemos abaixo: \\[ H_1: \\Delta \\neq 0 \\] Com os dados já ordenados em ordem crescente, calculamos \\(S_m\\) e \\(S_n\\), que serão as somas dos postos de X e Y , respectivamente. Após isso, obteremos: \\[ U_m = S_m - \\dfrac{1}{2}m(m+1) \\; \\text{e} \\; $U_n = S_n - \\dfrac{1}{2}n(n+1) \\] Após isso, calcula-se a estatistica de teste W, que será o menor valor entre \\(U_m\\) e \\(U_n\\). Sob \\(H_0\\), espera-se que tenhamos uma distribuição de postos aproximadamente igual, daí, o posto médio das duas amostras deve ser parecido. Daí,deve-se encontrar os valores críticos \\(t_1\\) e \\(t_2\\), de forma que \\[ P[W&lt;t_1] = P[W&gt;t_2] \\approx \\dfrac{\\alpha}{2} \\] Rejeita-se \\(H_0\\) caso \\(W_{obs}&lt;t_1\\) ou \\(W_{obs}&gt;t_2\\) Seu valor-p será dado por: \\[ 2P(W&gt;W_{obs}-1,\\text{ se $W_{obs}&gt;\\dfrac{mn}{2}$ ou $2P(W&lt;W_{obs})$, se $W_{obs}\\leq\\dfrac{mn}{2}$ } \\] "],["regressão.html", "Capítulo 4 Regressão 4.1 Modelos de Regressão 4.2 Coeficiente de Determinação 4.3 Transformação de Yeo-Johnson 4.4 Análise de Regressão Linear e Correlação 4.5 Análise de Variância Multivariada 4.6 Regressão Linear Múltipla", " Capítulo 4 Regressão 4.1 Modelos de Regressão Em diversos estudos é de extrema importância verificar se duas ou mais variáveis estão relacionadas entre si. Uma alternativa é determinar um modelo matemático que expresse tal relação, e na estatística este tipo de análise é conhecido como: Análise de Regressão. Em outras palavras, modelos de regressão nos ajudam a compreender melhor como uma variável influência no comportamento de outras. Caso o interesse da pesquisa seja avaliar a relação da variável dependente (expressa por \\(Y\\)) com apenas uma variável independente (\\(X\\)), estamos lidando com Regressão Linear Simples. Porém, se for necessária a verificação de duas ou mais variáveis independentes, temos um caso de Regressão Linear Múltipla. Os modelos de regressão múltipla são construídos pelos seguintes passos: Seleção de variáveis: a priori, não sabemos quais são as variáveis independentes que influenciam de forma mais significativa na variável resposta (dependente). O objetivo é encontrar um modelo mais parcimonioso que explica os dados, porém quanto mais variáveis no modelo, maior se torna a estimativa do erro e mais dependente o modelo fica dos dados observados. Então, devemos checar a importância das variáveis, incluindo ou excluindo-as do modelo se baseando em uma regra de decisão. Para esta seleção utiliza-se a relação das variáveis preditoras (inpedendentes) com a variável resposta (dependente), analisando sempre a relação de cada variável incluída, observando a significância da mesma. Estimação dos parâmetros: a estimação dos parâmetros significa obter valores (estimativas) para os mesmos, para que possamos incluir esses resultados no modelo. 3.Análise residual ou diagnóstico: auxilia no ajuste final do modelo, identificando observações que influênciam na estimação dos parâmetros e/ou mudança na reta dos ajustados. 4.2 Coeficiente de Determinação Uma das formas de avaliar a qualidade do ajuste do modelo é através do coeficiente de determinação, representado por \\(R^2\\). Este varia entre \\(0\\leq R^2 \\leq 1\\) e indica quanto o modelo foi capaz de explicar os dados coletados. Vale ressaltar que é pouco comum que tenhamos uma correlação perfeita (\\(R^2=1\\)) na prática, porque existem muitos fatores que determinam as relações entre variáveis na vida real. O coeficiente de determinação é dado pela seguinte expressão \\[ \\begin{eqnarray*} R^2=\\dfrac{\\bigg(\\sum_{i=1}^n (x_i-\\bar{x})Y_i\\bigg)^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (Y_i - \\bar{Y})^2} \\end{eqnarray*} \\] 4.3 Transformação de Yeo-Johnson Existem situações que a variável em estudo não possui comportamento normal e para o uso da regressão linear múltipla é necessário que a suposição de normalidade seja cumprida. Uma alternativa em experimentos como esse é o uso de alguma transformação na variável resposta. A transformação de Box-Cox (1964) é amplamente utilizada, contudo essa transformação é válida apenas em variáveis positivas. Uma alternativa de transformação para variáveis que assumem valores positivos e negativos é a transformação de Yeo-Johnson (2000), essa é uma extensão da transformação de Box-Cox. Sua fórmula é definida a seguir \\[ \\begin{eqnarray*} \\phi^{(\\lambda)}=\\left\\{\\begin{array}{rc} \\dfrac{(y+1)^\\lambda-1}{\\lambda},&amp;\\mbox{para }\\lambda \\neq0, y \\geq 0\\\\ \\text{log}(y+1), &amp;\\mbox{para } \\lambda =0 , y\\geq 0 \\\\ -\\dfrac{(1-y)^{2-\\lambda}-1}{2-\\lambda}, &amp;\\mbox{para } \\lambda \\neq2 , y &lt; 0 \\\\ -\\text{log}(1-y), &amp;\\mbox{para } \\lambda =2 , y &lt; 0 \\end{array}\\right. \\end{eqnarray*} \\] em que \\(\\lambda\\) é um parâmetro desconhecido e \\(\\phi^{(\\lambda)}\\) é a observação transformada. 4.4 Análise de Regressão Linear e Correlação A análise de regressão linear consiste em estudar a relação de uma variável dependente(variável resposta) e variáveis independentes(variáveis de regressão), essas variáveis são denominadas variável \\(y\\) e \\(x\\), respectivamente, e essa relação é expressa na forma funcional abaixo descrita a seguir: \\[Y = \\beta_{0} + \\beta_{1} X_{1} + \\ldots + \\beta_{p} X_{n}\\] Escrever a variável resposta em função da(s) variável(eis) \\(x\\) implica dizer que existe uma relação linear entre elas. Cada coeficiente do modelo é estimado por meio do método de máxima verossimilhança, o qual obtém-se estimadores com boas propriedades. Quanto a adequabilidade do modelo pode-se testar a partir da análise de variância (ANOVA) se o modelo é adequado ou não. Para isso as hipóteses testadas são \\[ \\begin{cases} H_0: \\beta_{0} = \\beta_{1} = \\ldots = \\beta_{p} = 0; \\\\ H_1: \\text{Pelo menos um} \\; \\beta_{j} \\; \\text{é diferente de zero.} \\end{cases} \\] A estatística F é usada para testar essas hipóteses. Tomando a seguinte regra de decisão se a estatística \\(F_{calculada} &gt; F_{tabelada}\\) rejeita-se a hipótese \\(H_{0}\\). Ou se o valor-p for obtido no teste for menor que o alfa de \\(5\\%\\). Rejeitar a hipótese \\(H_{0}\\) significa que as co-variáveis são significativas para explicar linearmente a variável resposta. 4.5 Análise de Variância Multivariada 4.5.1 (ANOVA) Análise de variância Assim como o teste t, essa técnica estatística compara a média de grupos, entretanto, enquanto o teste t focaliza em dois grupos, por outro lado, a ANOVA compara três ou mais grupos e, adicionalmente, assume que as variâncias são iguais em todos os grupos (homocedasticidade). A fim de testar a igualdade das médias, utiliza-se a seguinte estatística de teste: \\[F = \\frac{S^2_{entre}}{S^2_{dentro}}\\] Onde o numerador é a variância entre os grupos e o denominador a variância dentro dos grupos para \\(k-1\\) e \\(n - k\\) graus de liberdade, respectivamente, em que \\(k\\) é o número de grupos e \\(n\\) o número de observações. 4.5.2 MANOVA A análise de variância multivariada (MANOVA) é uma forma generalizada da análise de variância (ANOVA). É utilizada em casos onde existem duas ou mais variáveis dependentes. A ferramenta MANOVA permite comparar se há diferença entre os tratamentos para as variáveis respostas. É utilizada a estatística Wilks para testar a igualdade entre os tratamentos, as hipóteses do teste são \\[\\begin{cases} H_0: \\mu_{1}=\\mu_{2}=\\ldots=\\mu_{k}; \\\\ H_1: \\text{pelo menos duas são diferentes.} \\end{cases} \\] em que $ _{i} $ , \\(i = 1,2,\\ldots,k\\) são as médias dos tratamentos. A estatística \\(\\Lambda^{*}\\) foi originalmente proposta por Wilks e corresponde a uma forma equivalente do teste F da hipótese de ausência de efeito de tratamento do caso uni-variado. Que é dada por: \\[ \\Lambda^{*} = \\dfrac{|E|}{|H|+|E|}\\] Onde, o determinante da soma do quadrado dos erros e dos produtos cruzados, a matriz W é dividida pelo determinante da soma total de quadrados e matriz de produtos cruzados T = H + E. Se H é grande em relação a E, então | H + E | será grande em relação a | E |. Assim, vamos rejeitar a hipótese nula quanto menor for a estatística de Wilks (perto de zero). 4.6 Regressão Linear Múltipla Podemos definir um modelo de regressão linear múltipla da seguinte maneira: \\[ \\begin{eqnarray*} Y= \\beta_0 + \\boldsymbol{\\beta}_j\\boldsymbol{X}_j+\\boldsymbol{\\varepsilon}_i,\\ i=1,...,n, \\end{eqnarray*} \\] em que \\(j=1,...,p\\) e \\(p\\) é o número de parâmetros do modelo. \\(Y\\) é um vetor \\(n\\times 1\\) correspondente a variável resposta do estudo; \\(\\beta_0\\) e \\(\\boldsymbol{\\beta}\\) correspondem aos parâmetros do modelo, e \\(\\boldsymbol{\\beta}\\) é um vetor \\(p\\times 1\\); \\(\\boldsymbol{X}\\) é a matriz de planejamento \\(n\\times p\\) correspondente às variáveis independentes do modelo; \\(\\boldsymbol{\\varepsilon}_i\\) corresponde ao erro experimental, do qual não podemos controlar. Também é conhecido como resíduo. É importante ressaltar que neste relatório iremos expressar qualquer matriz ou vetor por letras em negrito. Além disso, para o uso desses modelos é preciso que algumas suposições sejam aceitas, tais como: Os erros (ou resíduos) devem seguir uma distribuição Normal e serem independentes; Os erros devem possuir médias iguais a zero e serem homocedásticos (variâncias constantes para cada indivíduo). "],["multivariada.html", "Capítulo 5 Multivariada", " Capítulo 5 Multivariada "],["amostragem.html", "Capítulo 6 Amostragem", " Capítulo 6 Amostragem "],["referências.html", "Capítulo 7 Referências", " Capítulo 7 Referências Agresti, A. (2002). Categorical data analysis. Second edition. New York: Wiley. Pages 91101 Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd ed. New York: John Wiley &amp; Sons. Page 38 Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd ed. New York: John Wiley &amp; Sons. Page 29 Bonferroni, C. E., Teoria statistica delle classi e calcolo delle probabilità, Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze 193 Bussab,W. O.; Morettin, P. A.Estatística Básica - 7 Edição, Atual Editora, 1987. R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. Lance, C.E., &amp; Vandenberg, R.J. (Eds.). (2014). More Statistical and Methodological Myths and Urban Legends: Doctrine, Verity and Fable in Organizational and Social Sciences (1st ed.). Routledge. https://doi.org/10.4324/9780203775851 Associação entre variáveis.Disponível em: http://sweet.ua.pt/andreia.hall/TEA/Capcorrel.pdf. Acesso:10 de janeiro 2018. Bussab,W. O.; Morettin, P. A.Estatística Básica - 7 Edição, Atual Editora, 1987. Bolfarine, H.;Sandoval, M. C. Introdução à Inferência Estatística} - 2 Edição, Editora SBM, 2010. Teste de hipótese. Disponível em:http://www.ufscar.br/jcfogo/Estat_2/arquivos/Teste_Hipotese.pdf. Acesso:12 de janeiro 2018. Viali,L.Apostila Testes Hipóteses Não Paramétricos.Porto Alegre, 2008. HANIFAN, Lyda J. The rural school community center. The Annals of the American Academy of Political and Social Science, v. 67, n. 1, p. 130-138, 1916. JOHNSON, R A. WICHERN, D. W. Applied Multivariate Statistical Analysis. Vol 5. No. 8. Upper Saddle River, NJ: Prentice hall, 2002. DE PÁDUA BRAGA, Antônio; DE LEON FERREIRA, André Carlos Ponce; LUDERMIR, Teresa Bernarda. Redes neurais artificiais: teoria e aplicações. Rio de Janeiro, Brazil:: LTC Editora, 2007. HAYKIN, Simon. Redes neurais: princípios e prática. Bookman Editora, 2007.. MORETTIN, Pedro Alberto; BUSSAB, WILTON OLIVEIRA. Estatística básica. Editora Saraiva, 2017. PAULA, Gilberto Alvarenga. Modelos de regressão: com apoio computacional. São Paulo: IME-USP, 2004. Scikit-Learn, Multilayer Perceptron. Disponível em:https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html. "]]
