[["index.html", "Metodologia Gauss Um pouco sobre Estatística Capítulo 1 Bem-vindo", " Metodologia Gauss Um pouco sobre Estatística Capítulo 1 Bem-vindo Universidade federal de Fortaleza | Departamento de Estatística e matemática aplicada \\[\\\\[1in]\\] Este é um livro para facilitar a vida dos Gaussianos com as metodologias para qualquer tipo de trabalhos. Para isso foi feito no github com todos as informações desse livro e foi feito site para disponibiliza-lo online de formar rapida e acessivel a todos, cuide bem desse material. "],["análise-descritiva.html", "Análise Descritiva 1.1 Medidas de Posição 1.2 Medidas de Dispersão 1.3 Tabela de contingência 1.4 Tabela de classes", " Análise Descritiva 1.1 Medidas de Posição 1.1.1 Média Em um conjuto de dados temos um número \\(n\\) de valores e a soma total desse conjuto de dados e a sua divisão pelo o valor de \\(n\\) nos dá a média, porém a média é influênciada por valores que fogem do padrão da amostra sendo o valor grande demais ou muito pequeno ,lembrando que a média pode ou não representar \\(50\\%\\) dos dados. Seja \\({\\displaystyle n}\\) o número total de valores e \\({\\displaystyle x_{i}}\\) cada valor, em que \\({\\displaystyle i=1,\\dots ,n}\\). Média aritmética é a soma dos valores \\({\\displaystyle x_{i}}\\) dividido pelo número total de valores \\({\\displaystyle n}\\): Formula : \\[\\bar{x} = \\dfrac{x_1 + \\ldots +x_n}{n} = \\frac{1}{n}\\sum_{i-1}^{n}x_i \\] 1.1.2 Mediana O termo mediana refere-se a meio. Dado um conjunto de informações numéricas, o valor central corresponde à mediana desse conjunto. Dessa forma, é importante que esses valores sejam colocados em ordem, seja crescente ou decrescente. Se houver uma quantidade ímpar de valores numéricos, a mediana será o valor central do conjunto numérico. Se a quantidade de valores for um número par, devemos fazer uma média aritmética dos dois números centrais, e esse resultado será o valor da mediana. 1.1.3 Moda A moda é a realização mais frequente em um conjunto de valores. O fenômeno acontece quando, em um banco de dados, há a repetição das informações encontradas em uma mesma variável. Uma empresa de Tecnologia da Informação tem 20 funcionários contratados. A maioria deles (15) tem idade entre 20 e 25 anos. O restante (5) está na faixa de 30 e 40 anos. A maior recorrência de idade, no entanto, é de funcionários com 23 anos. São 5 no total  definindo a medida de posição ora apresentada. A moda pode ser definida em bimodal (quando ocorre a repetição de dois valores) ou multimodal (mais de dois valores repetidos). O caso dos cinco funcionários com 23 anos identificamos como multimodal. 1.1.4 Quartil Temos que os Quartis são valores de divisão na qual divide os conjuntado de dados em 4 partes, na qual temos: Primeiro Quartil divide em \\(25\\%\\) em uma amostra ordena os valores inferiores. Segundo Quartil ou Mediana divide em \\(50\\%\\) em uma amostra ordena os valores inferiores. Terceiro Quartil divide em \\(75\\%\\) em uma amostra ordena os valores superiores. São utilizados para entender como se comporta os dados em cada quartil correspondente até aquele ponto. Intervalo interquartil(IIQ) avalia a dispersão de dados somente depois de ordená-los em ordem crescente. O intervalo interquartil é calculado com base no cálculo de quartis, sendo o primeiro quartil (inferior), o quartil intermediário (mediana), o terceiro quartil (superior), que estão ligados ao conceito de quantil. A diferença entre o quartil superior e o quartil inferior determina o intervalo interquartil 1.2 Medidas de Dispersão 1.2.1 Amplitude 1.2.2 Variância A variância é determinada pela média dos quadrados das diferenças entre cada uma das observações e a média aritmética da amostra. O cálculo é feito com base na seguinte fórmula: Variancia Amostral Variância Populacional \\[ \\sigma ^{2}={\\frac {1}{N}}\\sum _{{i=1}}^{N}\\left(x_{i}-\\mu \\right)^{2}, \\] Sendo, \\(\\sigma^2\\) : variância \\(x_i\\): valor observado \\(bar(x)\\) : média aritmética da amostra n: número de dados observados 1.2.3 Desvio Padrão \\[ \\sigma(S) = \\sqrt{Variância} \\] 1.2.4 Coeficiente de Variação \\[cv = \\dfrac{\\sigma}{\\bar(x)} \\] 1.3 Tabela de contingência As tabelas de contingência são usadas para registrar observações independentes de duas ou mais variáveis aleatórias, normalmente qualitativas. Suponha que tenhamos duas variáveis de uma população, A e B, e queremos relacioná-la com outras duas variáveis C e D . Retirando-se uma amostra aleatória dessa população, uma tabela de contingência conteria as frequências em cada classe, a tabela seria da seguinte forma: Variáveis Variáveis C D A 5 3 B 3 49 Total 8 52 Em posse da tabela de contingência podemos então realizar testes para saber se há ou não independência entre variáveis. 1.3.1 Teste de McNemar O teste de McNemar é utilizado em tabelas de contingência, essencialmente em tabelas \\((2\\times2)\\), com dados pareados para comparar frequências marginais, que em outras palavras significa que o teste compara se houve mudança nas proporções no objeto de estudo antes e depois de uma intervenção. As hipóteses do teste são: \\[ \\left\\{ \\begin{array}{l} H_0: \\text{As variáveis linha e coluna possuem as mesmas proporções},\\\\ H_1: \\text{As variáveis linha e coluna possuem proporções diferentes}. \\\\ \\end{array} \\right. \\] A estatística do teste, \\(Q\\), é dada por: \\[ Q=\\cfrac{(a-d)^2}{(a+d)} \\] em que \\(Q \\sim \\chi^2(1)\\), lê-se \\(Q\\) segue uma distribuição qui-quadrado com 1 grau de liberdade. 1.4 Tabela de classes A tabela de classes é usada quando temos dados brutos provenientes de uma variável contínua, e então nós as agrupamos para a construção de uma tabela, em intervalos que também são conhecidos por classes. Suponhamos que desejamos construir K classes. O valor mínimo da nossa amostra(mín) e o máximo(máx). A partir dessas informações, calculamos a amplitude total(AT): \\[ \\text{AT = máx  mín.} \\] Como o número de classes (k) é dada por: \\[ k = 1+3,3$\\log(n)$. \\] A amplitude de cada classe (h) é dada por: \\[ h = \\frac{AT}{k}. \\] "],["análise-de-correlação.html", "Análise de Correlação 1.5 Coeficiente de Cramér 1.6 Coeficiente de Correlação Ponto Biserial 1.7 Correlação linear de Pearson 1.8 Coeficiente de correlação de Spearman 1.9 Coeficiente \\(\\tau\\) de Kendall", " Análise de Correlação 1.5 Coeficiente de Cramér O coeficiente de Cramér é uma medida entre duas variáveis medidas numa escala categórica. Portanto pode ser aplicado em situações onde a informação se encontra distribuída por categorias nominais não ordenáveis. Este coeficiente obtém-se diretamente a partir da estatística \\(\\chi_v^2\\) através da expressão \\[ C = \\sqrt{\\dfrac{\\chi_v^2}{n(l-1)}} \\] onde n representa o número total de observações e l representa o mínimo entre o número de linhas e colunas na tabela de contingência. A partir do valor do coeficiente de Cramér é possível efetuar um teste às hipóteses \\[ \\begin{cases} H_0:\\text{ As variáveis são independentes;} \\\\ H_1:\\text{ As variáveis não são independentes.} \\end{cases} \\] 1.6 Coeficiente de Correlação Ponto Biserial Esse coeficiente (\\(r_{pb}\\)) mede a correlação entre uma variável quantitativa e uma variável categórica binária (0,1). Ele pode variar entre -1 e 1 e é definido pela seguinte fórmula: \\[ r_{pb} = \\dfrac{M_1-M_0}{S_n}\\sqrt{pq} \\] \\(M_1\\) = média do grupo que recebeu o valor 1 da variável binária. \\(M_0\\) = média do grupo que recebeu o valor 0 da variável binária. \\(S_n\\) = desvio padrão da variável binária toda. \\(p\\) = proporção de casos do grupo 0 na variável binária. \\(q\\) = proporção de casos do grupo 1 na variável binária. 1.7 Correlação linear de Pearson De modo geral, a quantificação do grau de associação entre duas variáveis é feita pelos chamados coeficientes de associação ou correlação. Essas são medidas que descrevem numericamente a associação (ou dependência) entre duas variáveis. Ou seja, tais resultados nos permitem verificar se uma variável influencia de forma positiva ou negativa na outra. Para facilitar a compreensão, esses coeficientes usualmente variam entre -1 e +1, e quanto mais próximo de 0, menos relação as variáveis possuem. A intensidade da associação linear existente entre as variáveis quantitativas pode ser determinada através do chamado Coeficiente de Correlação Linear de Pearson, e sua fórmula é definida por: \\[\\rho_{(x,y)} = \\frac{cov{(x,y)}}{S_{x}S_{y}},\\] em que , \\(\\text{Cov}{(x,y)}\\) - Covariância ou variância conjunta das variáveis X e Y; \\(S_{x}\\)- Desvio Padrão da variável X \\(S_{Y}\\) - Desvio Padrão da variável Y. Como dito anteriormente, o \\(\\rho\\) pode assumir valores entre -1 e +1 e neste ponto cabe algumas observações, tais como \\(\\rho_{(x,y)}\\) = +1 significa uma correlação positiva perfeita entre as duas variáveis, ou seja, se uma aumenta a outra também aumenta; \\(\\rho_{(x,y)}\\) = -1 significa uma correlação negativa perfeita entre as duas variáveis, isto é, se uma aumenta a outra diminui; \\(\\rho_{(x,y)}\\) = 0 significa que as duas variáveis não dependem linearmente uma da outra. No entanto, pode existir uma outra dependência não-linear. Logo, o resultado deve ser investigado por outros meios. 1.8 Coeficiente de correlação de Spearman O coeficiente de Correlação de Spearman é geralmente utilizado quando temos duas variáveis ordinais ou quando não podemos utilizar o teste Cramér V, pois não podemos facilmente atestar suas suposições. Diante de duas variáveis medidas numa escala de ordenação clara, ou que apresentam uma relação não linear mas monótona (se uma aumenta a outra tem sempre tendência a aumentar (ou a diminuir)). É um método não-paramétrico que usa os postos das variáveis quando o coeficiente de correlação de Pearson não pode ser aplicado, temos como alternativa o coeficiente de correlação de Spearman. A ideia de construção deste coeficiente é bastante simples. Dadas duas amostras de observação ordenáveis, substitui-se cada um dos seus valores pela sua ordem de ordenação, em inglês Por exemplo, se uma amostra de três valores for \\(x_{1} = 2.1\\), \\(x_{2} = 1.7\\), \\(x_{3} = 4.8\\), então os respectivos ranks serão \\(r_{1}= 2\\), \\(r_{2} = 1\\), \\(r_{3} = 3\\). Após substituir cada uma das amostras pelos seus ranks o coeficiente de Spearman não e mais do que o coeficiente de Pearson aplicado agora aos . Uma vez que as ordens variam sempre entre 1 e \\(n\\) (número de observações), pode-se escrever o coeficiente do seguinte modo \\[ R_{s}=1- \\dfrac{ 6\\sum_{i=1}^{n} D_{i}^2}{n^{3} - n}, \\] onde \\(D_{i}\\) representa a diferença de ranks entre as observações que estão sendo analisadas. 1.9 Coeficiente \\(\\tau\\) de Kendall Uma alternativa ao coeficiente de Spearman é o coeficiente de correlação \\(\\tau\\) de Kendall que se aplica nas mesmas condições, porém com duas vantagens : se as amostras tiverem dimensão muito reduzida e valores repetidos, os resultados do teste são mais precisos; por outro lado, o coeficiente \\(\\tau\\) de Kendall pode ser generalizado para correlações parciais que são correlações medidas entre duas variáveis após remoção do efeito de uma possível terceira variável sobre ambas. O coeficiente de Kendall é descrito como uma medida de concordância entre dois conjuntos, medindo a diferença entre a probabilidade de as classificações estarem na mesma ordem e a probabilidade de estarem em ordens diferentes. \\[ \\tau = \\dfrac{ concordantes - discordantes}{\\frac{n(n-1)}{2}}. \\] "],["inferência.html", "Capítulo 2 Inferência 2.1 Teste de Hipóteses 2.2 Testes de Independência 2.3 Teste T para amostras independentes 2.4 Teste Exato de Fisher 2.5 Teste de Bonferroni 2.6 Teste de Proporção 2.7 Teste de Post Hoc 2.8 Teste de McNemar 2.9 Teste de Wilcoxon pareado 2.10 Teste Qui-Quadrado", " Capítulo 2 Inferência 2.1 Teste de Hipóteses 2.1.1 Definição Um teste de hipóteses estatístico é um procedimento que utilizamos para testar duas hipóteses disjuntas a respeito de um parâmetro de interesse. Utiliza-se a nomenclatura \\(H_0\\) e \\(H_1\\) para representar as hipóteses, as quais são chamadas de hipótese nula e hipótese alternativa, respectivamente. Quando testamos uma hipótese, podemos encontrar dois tipos de erros: - Erro Tipo I: Consiste em rejeitarmos a hipótese nula, dado que ela é vera dadeira. Geralmente, representa-se por \\(\\alpha\\) a probabilidade de cometer esse tipo de erro. - Erro Tipo II: Consiste em não rejeitarmos \\(H_0\\), dado que ela é falsa. Normalmente representa-se com a letra grega \\(\\beta\\) a probabilidade de cometer esse erro. Ambos os erros devem ser evitados, porém, por questões matemáticas, geralmente só podemos controlar facilmente o Erro Tipo I, por isso, devemos estabelecer as hipóteses de forma que o Erro Tipo I seja o erro mais prejudicial ao processo estudado. Nível de significância: O valor de \\(\\alpha\\), referente ao Erro Tipo I, é chamado de nível de significância, esse valor deve ser pré-estabelecido pelo pesquisador. Estatística de teste: É a estatística amostral na qual basearemos seus valores para decidirmos pela rejeição ou não de \\(H_0\\). Essa estatística é associada ao estimador do parâmetro que se deseja testar. Por exemplo, ao queremos testar algo sobre o real valor da média populacional \\(\\mu\\) (parâmetro),quando conhece-se o valor do desvio padrão populacional (\\(\\sigma\\)) utiliza-se a a estatística de teste abaixo: \\[ Z = \\dfrac{\\bar{X}-\\mu}{\\sigma/\\sqrt n} \\] Onde: \\(\\bar{X}\\) é a média amostral, sendo ela o estimador natural da média populacional; n é o tamanho da nossa amostra. Região de Rejeição: É a região formada pelo conjunto de valores que levam \\(H_0\\) a ser rejeitada. O valor que delimita essa região é denominado valor crítico. Caso o valor calculado da estatística de teste caia nessa região, rejeita-se a hipótese nula. Nível descritivo ou valor-p: É o menor valor para o qual rejeita-se a hipótese nula. Quando esse valor cai na região crítica, rejeita-se \\(H_0\\). Para sabermos se rejeitaremos ou não \\(H_0\\) baseado no valor-p, deve-se compará-lo com o nível de significância do teste e rejeitar \\(H_0\\), se aquele for menor do que este, rejeita-se \\(H_0\\). 2.2 Testes de Independência Como em todo teste de hipóteses, devemos estabelecer primeiro as hipóteses a serem testadas. Num teste de independência essas hipóteses são: H_0: As variáveis de classificação são independentes. H_1: As variáveis de classificação não são independentes. Existem vários testes de independência em que cada um desses são adequados para uma determinada situação 2.3 Teste T para amostras independentes O teste T é utilizado para verificar a existência de diferença entre as médias de dois grupos da mesma população. Para isso a variável independente deve ser necessariamente métrica e a variável dependente deve ser categórica. Suponha que o objetivo seja analisar as médias amostrais \\(\\bar{x}_1\\) e \\(\\bar{x}_2\\) Assim, testa-se a hipótese como segue: \\[\\begin{cases} H_0: \\bar{x}_1 = \\bar{x}_2 \\\\ H_1: \\bar{x}_1 \\neq \\ \\bar{x}_2 \\end{cases}\\] Suponha que o objetivo seja analisar as médias amostrais \\(\\bar{x}_1\\) e \\(\\bar{x}_2\\) \\[t = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{s^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\] Onde o numerador é a diferença das médias amostrais e o denominados é o desvio padrão amostral 2.4 Teste Exato de Fisher O Teste Exato de Fisher é utilizado para verificar se existe associação entre as variáveis linha e as variáveis coluna em uma tabela de contingência construída a partir dos dados da amostra. Embora na prática o Teste Exato de Fisher seja aplicado quando os tamanhos das amostras são pequenos, o teste é válido para todos os tamanhos amostrais. As hipóteses do teste são: \\[ \\begin{cases} H_0: \\text{As variáveis linha e coluna são independentes}; \\\\ H_1: \\text{As variáveis linha e coluna não são independentes.} \\end{cases} \\] A estatística de teste, apresentada por Fisher (1934), propõe que a distribuição de probabilidade das frequências de quaisquer tabelas de contingência sejam substituídas pela probabilidade da distribuição das mesmas frequências. Considerando a Tabela , arranjada de modo que \\(n_{1.}\\leq n_{.1}\\leq n_{.2}\\leq n_{2.}\\), temos uma distribuição de probabilidade hipergeométrica para a única frequência de valor independente, \\(o_{11}\\), a partir de, \\[P[X=o_{11}]=\\cfrac{\\displaystyle{n_{1.} \\choose o_{11}} \\displaystyle{n_{2.} \\choose o_{21}}}{\\displaystyle{n \\choose n_{.1}}}=\\cfrac{n_{1.}!n_{2.}!n_{.1}!n_{.2}!}{o_{11}!o_{12}!o_{21}!o_{22}!n!}\\] O Teste Exato de Fisher consiste na determinação desta probabilidade e dos arranjos possíveis que, com os mesmos totais marginais, tenham ainda mais desvios em relação à hipótese nula \\(H_0\\). 2.5 Teste de Bonferroni O teste de bonferroni consiste na realização de um teste t para cada par de médias a uma taxa de erro por comparação (TPC) de \\(a/(k/2)\\). Usando esse teste, o nível de significância da família é no máximo \\(a\\), para qualquer média das populações. Assim o teste de Bonferroni protege a taxa de erro da família dos testes. Pode ser usado para quaisquer que sejam os dados balanceados ou não. Não é um teste exato, e assim é baseado na aproximação como primeira desigualdade de Bonferroni. Em diversos momentos ele pode ser um teste conservativo, isto é, a taxa de erro da família testes é muito menor que o nível de significância \\(a\\). Para tamanhos de amostras iguais o teste de Bonferroni considera duas médias significativamente diferentes se o valor absoluto das diferenças ultrapassar: \\[ LSD=t_{(a,N-k)}\\sqrt{2\\frac{QME}{n}} \\] E para os dados não balanceados temos: \\[ LSD=t_{(a,N-k)}\\sqrt{QME(\\frac{1}{n_i}+\\frac{1}{n_j})} \\] Em que \\(a^\\prime=\\frac{1}{2}(a/c)\\) e c é o número de comparações duas a duas. O quantil \\(t_{(a,N-k)}\\) é da distribuição t-Student com parâmetros \\(N-k\\). Assim a margem de erro da equação anterior depende do número de comparações. 2.6 Teste de Proporção Temos que testar hipóteses e construir intervalos de confiança para a proporção de uma população. Supondo que uma amostra aleatória de tamanho n tenha sido retirada de uma grande população e que X são as observações, nessa amostra pertencem a uma classe de interesse. Então \\(P=X/n\\) é um estimador da proporção \\(p\\) da população, que pertence a essa classe, \\(n\\) e \\(p\\) são parâmetros da Distribuição Binomial.,em que \\(n\\) é o tamanho amostral e \\(p\\) a proporção. Para o teste bilateral: \\[ \\begin{cases} H_0: p=p_0\\\\ H_1: p \\neq p_0 \\end{cases} \\] A estatística de teste é: \\[Z_0=\\frac{X-np_0}{\\sqrt{nX/n(1-X/n)}}\\] Rejeitamos a hipótese nula se \\[ Z_0&lt;- Z_{1-\\alpha/2} \\,\\text{ ou }\\, Z_0 &gt; Z_{1-\\alpha/2} \\] Para o teste unilateral a direita: \\[ \\begin{cases} H_0: p=p_0\\\\ H_1: p &gt; p_0 \\end{cases} \\] A estatística de teste é: \\[Z_0=\\frac{X-np_0}{\\sqrt{nX/n(1-X/n)}}\\] Rejeitamos a hipótese nula se \\[ Z_0 &gt; Z_{1-\\alpha} \\] 2.7 Teste de Post Hoc Análise Post Hoc é feita após a conclusão do estudo, usando o tamanho do estudo e de efeito obtidos para determinar a potência da amostra estudada, podemos dizer que são conjuntos de teste para determinar as diferenças presentes. Os testes são para comparações pareadas, projetadas afim de comparar todas as diferentes combinações dos grupos de tratamento. Observando as taxas erro, temos: Erro tipo 1: É a probabilidade de rejeitar a hipótese nula (Ho), quando ela é verdadeira. Erro tipo 2: É a probabilidade de não rejeitar a hipótese nula (Ho), quando ela é falsa. Probabilidade de acontecer pelo menos um erro do Tipo I em um conjunto de relações estatísticas, assumindo erroneamente que pelo menos uma das diferenças analisadas é significativamente diferente da hipótese nula. \\[ { \\alpha_{F_W}}= 1-(1- {\\alpha})^c \\] Onde C é o número de comparações, então dessa forma temos que: \\[ c=\\frac{k*(k-1)}{2} \\] 2.8 Teste de McNemar O teste de McNemar é utilizado em tabelas de contingência, essencialmente em tabelas \\((2\\times2)\\), com dados pareados para comparar frequências marginais, que em outras palavras significa que o teste compara se houve mudança nas proporções no objeto de estudo antes e depois de uma intervenção. As hipóteses do teste são: \\[ \\begin{cases} H_0: \\text{As variáveis linha e coluna possuem as mesmas proporções;}\\\\ H_1: \\text{As variáveis linha e coluna possuem proporções diferentes.} \\end{cases} \\] A estatística do teste, \\(Q\\), é dada por: \\[ Q=\\cfrac{(a-d)^2}{(a+d)}\\] em que \\(Q \\sim \\chi^2(1)\\), lê-se \\(Q\\) segue uma distribuição qui-quadrado com 1 grau de liberdade. 2.9 Teste de Wilcoxon pareado O teste de Wilcoxon pareado é utilizado para comparar se dois grupos possuem a mesma medida de tendência central. Esse teste leva em consideração a magnitude das diferenças entre os pares. Seja \\(d_i\\) o escore-diferença para qualquer par combinado, representando a diferença dos pares sobre dois tratamentos \\(X\\) e \\(Y\\), isto é, \\(d_i=X_i-Y_i\\). Ordena-se todos os \\(d_i\\)s sem considerar o sinal, posteriormente dar-se o posto 1 ao menor \\(|d_i|\\), posto 2 ao segundo menor, e assim sucessivamente. As possíveis hipóteses do teste são: \\(H_0\\): \\(d=0\\) vs \\(H_1\\): \\(d \\neq 0\\); \\(H_0\\): \\(d=0\\) vs \\(H_1\\): \\(d&gt;0\\); \\(H_0\\): \\(d=0\\) vs \\(H_1\\): \\(d&lt;0\\). A hipótese nula (\\(H_0\\)) é que os tratamentos \\(X\\) e \\(Y\\) são equivalentes, isto é, eles são amostras de populações com a mesma mediana e a mesma distribuição contínua. Duas estatísticas são definidas: \\[ T^+=\\sum_{i}d_i, \\] \\[ T^-=\\sum_{i}d_i, \\] soma dos \\(d_i\\)s positivos e soma dos \\(d_i\\)s negativos, respectivamente. Rejeitamos a hipótese nula se a probabilidade de \\(T^+\\) tabulado para um determinado tamanho \\(N\\) é menor ou igual ao nível de significância escolhido. 2.10 Teste Qui-Quadrado 2.10.1 Teste Qui-Quadrado de Pearson para independência O teste de independência Qui-Quadrado é usado para descobrir se existe uma associação entre a variável de linha e a variável de coluna em uma tabela de contingência construído à partir de dados da amostra. A hipótese nula é de que as variáveis não estão associadas, em outras palavras, eles são independentes. A hipótese alternativa é de que as variáveis estão associadas, ou dependentes. Para que o teste possa ser aplicado, é necessário levar em consideração algumas suposições e condições, que serão listadas a seguir: Os dados devem ser derivados de contagens(frequências) para as categorias das variáveis categóricas; As frequência das células da tabela de dupla entrada devem ser independentes umas das outras; Os sujeitos contados na tabela devem ser de uma amostra aleatória extraída de uma única população; Devemos ter dados suficientes; A tabela de contingência(dupla entrada) deve conter pelo menos 5 observações em cada célula; A tabela de valores esperados não pode conter observações menores que 5 O teste é aplicável tanto para a variável qualitativa quanto quantitativa. As hipóteses do teste são: \\[ \\begin{cases} H_0: \\text{As variáveis X e Y são independentes};\\\\ H_1: \\text{As variáveis X e Y não são independentes.} \\end{cases} \\] A Estatística do teste é dada por: \\[\\chi^2 = \\sum_i\\sum_j\\frac{(o_{ij} - e_{ij}^2)}{e_{ij}}\\] onde: - \\(o_{ij}\\) é o valor observado na célula; - \\(e_{ij}\\) é o valor esperado - \\(e_{ij} = \\frac{\\text{total da linha i} * \\text{total da coluna j}}{\\text{total geral}}\\) Rejeitamos \\(H_0\\) se \\(\\chi^2_{calculado} &gt; \\chi^2_{tabelado}\\) 2.10.2 Teste Qui-Quadrado O teste \\(\\chi^2\\) ( Qui- Quadrado) é um dos testes usados para avaliar se há independência entre variáveis qualitativas, para isso, devemos dispor nossos dados em uma tabela de contingência, no caso em que queremos testar a hipótese de independência entre exatamente duas variáveis, utiliza-se uma tabela de dupla entrada. Para realizar o teste, iremos testar a hipótese como segue: \\[\\begin{cases} H_0: \\text{As variáveis são independentes;} \\\\ H_1: \\text{As variáveis não são independentes.} \\end{cases} \\] Para isso, utilizaremos a estatística de teste abaixo: \\[ \\mathbf{\\chi^2_v} = \\dfrac{\\sum_{\\substack{i=1}}^{k} \\sum_{\\substack{i=1}}^{l}(O_{ij}-E_{ij})^2}{E_{ij}} \\] onde: k é o número de linhas da tabela ; l é o número de colunas; \\(E_{ij} = n p_{ij}\\) é a frequência esperada da célula ij; \\(p_{ij}\\) é a probabilidade de ocorrer uma observação na célula ij. Se as variáveis são independentes, espera-se que \\(p_{ij} = p_{i.}p_{j.}\\), onde \\(p_i{i.}\\) é a probabilidade marginal correspondente a linha i e \\(p_{.j}\\) é a probabilidade marginal correspondente a coluna j; \\(v = (k-1)(l-1)\\) é o grau de liberdade. Assim como nos demais testes de hipóteses, rejeitaremos \\(H_0\\) caso a probabilidade encontrada seja menor do que o nível de significância estipulado. Porém, esse teste apresenta problemas quando temos um número de observações considerado pequeno - geralmente considera-se pequeno um tamanho amostral menor do que 40 unidades - ou quando temos em uma das caselas de cruzamento um número esperado menor do que 5. Nesses casos, utiliza-se uma correção para uma melhor estimação. A correção indicada para esse caso é a de Yates ou de Continuidade, pela qual o valor do teste Qui-Quadrado será recalculado como se segue: \\[ \\mathbf{\\chi^2_v} = \\dfrac{\\sum_{\\substack{i=1}}^{k} \\sum_{\\substack{i=1}}^{l}(|O_{ij}-E_{ij}|-0,5)^2}{E_{ij}}. \\] "],["estatística-não-paramétrica.html", "Capítulo 3 Estatística Não-Paramétrica 3.1 Teste de Wilcoxon-Mann-Whitney", " Capítulo 3 Estatística Não-Paramétrica 3.1 Teste de Wilcoxon-Mann-Whitney 3.1.1 Definição 1 Proposto inicialmente por Frank Wilcoxon (1945) e com contribuições de Henry B. Mann e Donald R. Whitney (1947), o teste de Wilcoxon-Mann-Whitney é um teste não paramétrico aplicado em duas amostras independentes. Esse teste é baseado nos postos dos valores obtidos combinando-se as duas amostras. Isso é feito ordenando-se esses valores, do menor para o maior, independentemente do fato de qual população cada valor provém, caso haja observações repetidas atribui-se a média dos postos correspondentes. O teste de Wilcoxon-Mann-Whitney tem como objetivo testar se as distribuições possuem a mesma medida de tendência central. Seja \\(X_1\\), \\(X_2\\), , \\(X_m\\) uma amostra aleatória de \\(X\\) de modo que \\(X_j\\)s são independentes e identicamente distribuídos e \\(Y_1\\), \\(Y_2\\), , \\(Y_n\\) uma amostra aleatória de \\(Y\\) de modo que os \\(Y_i\\)s são independentes e identicamente distribuídos. Além disso, suponha que os \\(X_j\\)s e os \\(Y_i\\)s são mutuamente independentes e a amostra \\(Y\\) aquela com o menor tamanho amostral, isto é, \\(n \\leq m\\). As possíveis hipóteses do teste são: \\(H_0\\): \\(\\Delta=0\\) vs \\(H_1\\): \\(\\Delta\\neq0\\); \\(H_0\\): \\(\\Delta=0\\) vs \\(H_1\\): \\(\\Delta&gt;0\\); \\(H_0\\): \\(\\Delta=0\\) vs \\(H_1\\): \\(\\Delta&lt;0\\). em que: \\(\\Delta\\) = Estimador da diferença das posições. Para estimar a diferença \\(\\Delta\\) entre as medianas das populações, consideramos todas as \\(m \\times n\\) diferenças \\(y_i - x_j\\) ordenadas de forma crescente. O estimador \\(\\hat{\\Delta}\\) associado a estatística de Wilcoxon-Mann-Whitney é definido por \\(\\hat{\\Delta}= \\ \\hbox{mediana}\\{(y_i-x_j), \\ i=1,\\ldots, n; j=1,\\ldots,m\\}\\), chamado de pseudo-mediana. A estatística de teste, denotada por \\(W\\), é dada pelo mínimo de \\(U_m\\) e \\(U_n\\) \\[ \\begin{align*} U_m &amp;= S_m - \\frac{m(m+1)}{2} \\\\ U_n &amp;= S_n - \\frac{n(n+1)}{2} \\end{align*} \\] em que: \\(S_m\\) = Soma dos postos relacionados a amostra \\(X\\); \\(S_n\\) = Soma dos postos relacionados a amostra \\(Y\\). 3.1.2 Definição 2 : O teste de Wilcoxon-Mann-whitney é um teste não-paramétrico, ou seja, é um teste usado quando temos poucas ( ou nenhuma) evidências sobre a real distribuição dos dados. Ele se utiliza de passos idênticos aos que usamos para calcular um teste de hipóteses usual. Seja \\(P_1\\) e \\(P_2\\) duas população das quais não temos informações sobre suas distribuições, porém, temos que seus dados são qualitativo ordinais ou quantitativos. Retira-se uma amostra de cada uma dessas populações, sendo essas independentes. O procedimento utilizado nesse teste consiste em ordenarmos essas duas amostras de forma combinada, ou seja, ordena-se esses valores sem a necessidade de levar em consideração de qual população o dado provém. Com isso, testaremos se as duas populações têm a mesma mediana, ou seja, se suas distribuições são iguais em localização. Sendo \\(X_1,X_2,...,X_m\\) e \\(Y_1,Y_2,...,Y_n\\), amostras aleatórias independentes e identicamente distribuídas(i.i.d) das populações \\(P_1\\) e \\(P_2\\), respectivamente. Além disso, essas amostras também serão independentes entre si, sendo essas tomadas de forma que \\(n\\leq m\\). Sendo F e G as funções de distribuição correspondentes as populações \\(P_1\\) e \\(P_2\\), respectivamente, a hipótese nula é tal como segue: \\[ H_0: F(t) = G(t),\\text{para todo t} \\] Entretanto, podemos também considerar que Y tem a mesma distribuição de X+\\(\\Delta\\). Perante isso, a hipótese nula será: \\[ H_0: \\Delta = 0 \\] Daí, como estamos interessados apenas em haver diferença entre as medianas, independentemente se essa diferença é negativa ou positiva, utilizamos como hipótese alternativa, uma hipótese bilateral, como vemos abaixo: \\[ H_1: \\Delta \\neq 0 \\] Com os dados já ordenados em ordem crescente, calculamos \\(S_m\\) e \\(S_n\\), que serão as somas dos postos de X e Y , respectivamente. Após isso, obteremos: \\[ U_m = S_m - \\dfrac{1}{2}m(m+1) \\; \\text{e} \\; $U_n = S_n - \\dfrac{1}{2}n(n+1) \\] Após isso, calcula-se a estatistica de teste W, que será o menor valor entre \\(U_m\\) e \\(U_n\\). Sob \\(H_0\\), espera-se que tenhamos uma distribuição de postos aproximadamente igual, daí, o posto médio das duas amostras deve ser parecido. Daí,deve-se encontrar os valores críticos \\(t_1\\) e \\(t_2\\), de forma que \\[ P[W&lt;t_1] = P[W&gt;t_2] \\approx \\dfrac{\\alpha}{2} \\] Rejeita-se \\(H_0\\) caso \\(W_{obs}&lt;t_1\\) ou \\(W_{obs}&gt;t_2\\) Seu valor-p será dado por: \\[ 2P(W&gt;W_{obs}-1,\\text{ se $W_{obs}&gt;\\dfrac{mn}{2}$ ou $2P(W&lt;W_{obs})$, se $W_{obs}\\leq\\dfrac{mn}{2}$ } \\] "],["regressão.html", "Capítulo 4 Regressão 4.1 Modelos de Regressão 4.2 Coeficiente de Determinação 4.3 Transformação de Yeo-Johnson 4.4 Análise de Regressão Linear e Correlação 4.5 Análise de Variância Multivariada 4.6 Regressão Linear Múltipla", " Capítulo 4 Regressão 4.1 Modelos de Regressão Em diversos estudos é de extrema importância verificar se duas ou mais variáveis estão relacionadas entre si. Uma alternativa é determinar um modelo matemático que expresse tal relação, e na estatística este tipo de análise é conhecido como: Análise de Regressão. Em outras palavras, modelos de regressão nos ajudam a compreender melhor como uma variável influência no comportamento de outras. Caso o interesse da pesquisa seja avaliar a relação da variável dependente (expressa por \\(Y\\)) com apenas uma variável independente (\\(X\\)), estamos lidando com Regressão Linear Simples. Porém, se for necessária a verificação de duas ou mais variáveis independentes, temos um caso de Regressão Linear Múltipla. Os modelos de regressão múltipla são construídos pelos seguintes passos: Seleção de variáveis: a priori, não sabemos quais são as variáveis independentes que influenciam de forma mais significativa na variável resposta (dependente). O objetivo é encontrar um modelo mais parcimonioso que explica os dados, porém quanto mais variáveis no modelo, maior se torna a estimativa do erro e mais dependente o modelo fica dos dados observados. Então, devemos checar a importância das variáveis, incluindo ou excluindo-as do modelo se baseando em uma regra de decisão. Para esta seleção utiliza-se a relação das variáveis preditoras (inpedendentes) com a variável resposta (dependente), analisando sempre a relação de cada variável incluída, observando a significância da mesma. Estimação dos parâmetros: a estimação dos parâmetros significa obter valores (estimativas) para os mesmos, para que possamos incluir esses resultados no modelo. 3.Análise residual ou diagnóstico: auxilia no ajuste final do modelo, identificando observações que influênciam na estimação dos parâmetros e/ou mudança na reta dos ajustados. 4.2 Coeficiente de Determinação Uma das formas de avaliar a qualidade do ajuste do modelo é através do coeficiente de determinação, representado por \\(R^2\\). Este varia entre \\(0\\leq R^2 \\leq 1\\) e indica quanto o modelo foi capaz de explicar os dados coletados. Vale ressaltar que é pouco comum que tenhamos uma correlação perfeita (\\(R^2=1\\)) na prática, porque existem muitos fatores que determinam as relações entre variáveis na vida real. O coeficiente de determinação é dado pela seguinte expressão \\[ \\begin{eqnarray*} R^2=\\dfrac{\\bigg(\\sum_{i=1}^n (x_i-\\bar{x})Y_i\\bigg)^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (Y_i - \\bar{Y})^2} \\end{eqnarray*} \\] 4.3 Transformação de Yeo-Johnson Existem situações que a variável em estudo não possui comportamento normal e para o uso da regressão linear múltipla é necessário que a suposição de normalidade seja cumprida. Uma alternativa em experimentos como esse é o uso de alguma transformação na variável resposta. A transformação de Box-Cox (1964) é amplamente utilizada, contudo essa transformação é válida apenas em variáveis positivas. Uma alternativa de transformação para variáveis que assumem valores positivos e negativos é a transformação de Yeo-Johnson (2000), essa é uma extensão da transformação de Box-Cox. Sua fórmula é definida a seguir \\[ \\begin{eqnarray*} \\phi^{(\\lambda)}=\\left\\{\\begin{array}{rc} \\dfrac{(y+1)^\\lambda-1}{\\lambda},&amp;\\mbox{para }\\lambda \\neq0, y \\geq 0\\\\ \\text{log}(y+1), &amp;\\mbox{para } \\lambda =0 , y\\geq 0 \\\\ -\\dfrac{(1-y)^{2-\\lambda}-1}{2-\\lambda}, &amp;\\mbox{para } \\lambda \\neq2 , y &lt; 0 \\\\ -\\text{log}(1-y), &amp;\\mbox{para } \\lambda =2 , y &lt; 0 \\end{array}\\right. \\end{eqnarray*} \\] em que \\(\\lambda\\) é um parâmetro desconhecido e \\(\\phi^{(\\lambda)}\\) é a observação transformada. 4.4 Análise de Regressão Linear e Correlação A análise de regressão linear consiste em estudar a relação de uma variável dependente(variável resposta) e variáveis independentes(variáveis de regressão), essas variáveis são denominadas variável \\(y\\) e \\(x\\), respectivamente, e essa relação é expressa na forma funcional abaixo descrita a seguir: \\[Y = \\beta_{0} + \\beta_{1} X_{1} + \\ldots + \\beta_{p} X_{n}\\] Escrever a variável resposta em função da(s) variável(eis) \\(x\\) implica dizer que existe uma relação linear entre elas. Cada coeficiente do modelo é estimado por meio do método de máxima verossimilhança, o qual obtém-se estimadores com boas propriedades. Quanto a adequabilidade do modelo pode-se testar a partir da análise de variância (ANOVA) se o modelo é adequado ou não. Para isso as hipóteses testadas são \\[ \\begin{cases} H_0: \\beta_{0} = \\beta_{1} = \\ldots = \\beta_{p} = 0; \\\\ H_1: \\text{Pelo menos um} \\; \\beta_{j} \\; \\text{é diferente de zero.} \\end{cases} \\] A estatística F é usada para testar essas hipóteses. Tomando a seguinte regra de decisão se a estatística \\(F_{calculada} &gt; F_{tabelada}\\) rejeita-se a hipótese \\(H_{0}\\). Ou se o valor-p for obtido no teste for menor que o alfa de \\(5\\%\\). Rejeitar a hipótese \\(H_{0}\\) significa que as co-variáveis são significativas para explicar linearmente a variável resposta. 4.5 Análise de Variância Multivariada 4.5.1 (ANOVA) Análise de variância Assim como o teste t, essa técnica estatística compara a média de grupos, entretanto, enquanto o teste t focaliza em dois grupos, por outro lado, a ANOVA compara três ou mais grupos e, adicionalmente, assume que as variâncias são iguais em todos os grupos (homocedasticidade). A fim de testar a igualdade das médias, utiliza-se a seguinte estatística de teste: \\[F = \\frac{S^2_{entre}}{S^2_{dentro}}\\] Onde o numerador é a variância entre os grupos e o denominador a variância dentro dos grupos para \\(k-1\\) e \\(n - k\\) graus de liberdade, respectivamente, em que \\(k\\) é o número de grupos e \\(n\\) o número de observações. 4.5.2 MANOVA A análise de variância multivariada (MANOVA) é uma forma generalizada da análise de variância (ANOVA). É utilizada em casos onde existem duas ou mais variáveis dependentes. A ferramenta MANOVA permite comparar se há diferença entre os tratamentos para as variáveis respostas. É utilizada a estatística Wilks para testar a igualdade entre os tratamentos, as hipóteses do teste são \\[\\begin{cases} H_0: \\mu_{1}=\\mu_{2}=\\ldots=\\mu_{k}; \\\\ H_1: \\text{pelo menos duas são diferentes.} \\end{cases} \\] em que $ _{i} $ , \\(i = 1,2,\\ldots,k\\) são as médias dos tratamentos. A estatística \\(\\Lambda^{*}\\) foi originalmente proposta por Wilks e corresponde a uma forma equivalente do teste F da hipótese de ausência de efeito de tratamento do caso uni-variado. Que é dada por: \\[ \\Lambda^{*} = \\dfrac{|E|}{|H|+|E|}\\] Onde, o determinante da soma do quadrado dos erros e dos produtos cruzados, a matriz W é dividida pelo determinante da soma total de quadrados e matriz de produtos cruzados T = H + E. Se H é grande em relação a E, então | H + E | será grande em relação a | E |. Assim, vamos rejeitar a hipótese nula quanto menor for a estatística de Wilks (perto de zero). 4.6 Regressão Linear Múltipla Podemos definir um modelo de regressão linear múltipla da seguinte maneira: \\[ \\begin{eqnarray*} Y= \\beta_0 + \\boldsymbol{\\beta}_j\\boldsymbol{X}_j+\\boldsymbol{\\varepsilon}_i,\\ i=1,...,n, \\end{eqnarray*} \\] em que \\(j=1,...,p\\) e \\(p\\) é o número de parâmetros do modelo. \\(Y\\) é um vetor \\(n\\times 1\\) correspondente a variável resposta do estudo; \\(\\beta_0\\) e \\(\\boldsymbol{\\beta}\\) correspondem aos parâmetros do modelo, e \\(\\boldsymbol{\\beta}\\) é um vetor \\(p\\times 1\\); \\(\\boldsymbol{X}\\) é a matriz de planejamento \\(n\\times p\\) correspondente às variáveis independentes do modelo; \\(\\boldsymbol{\\varepsilon}_i\\) corresponde ao erro experimental, do qual não podemos controlar. Também é conhecido como resíduo. É importante ressaltar que neste relatório iremos expressar qualquer matriz ou vetor por letras em negrito. Além disso, para o uso desses modelos é preciso que algumas suposições sejam aceitas, tais como: Os erros (ou resíduos) devem seguir uma distribuição Normal e serem independentes; Os erros devem possuir médias iguais a zero e serem homocedásticos (variâncias constantes para cada indivíduo). "],["multivariada.html", "Capítulo 5 Multivariada 5.1 Redes Neurais Artificiais 5.2 Análise de Componentes Principais", " Capítulo 5 Multivariada 5.1 Redes Neurais Artificiais Redes neurais artificiais é um método baseado na estrutura de funcionamento dos neurônios do cérebro humano. A arquitetura do método é semelhante a todo o processo que ocorre com os neurônios, quanto a forma de aprendizado e repasse de informação entre neurônios através dos dendritos. A estrutura básica de um neurônio biológico é composto por três seções: o corpo da célula, os dendritos e o axônio. A figura 2.1 é uma ilustração da estrutura de um neurônio biológico A partir de inúmeros estudos na área, um modelo em particular baseado em neurônio biológico foi proposto por Warren S. McCulloch e Walter Pitts no ano de 1943 em um artigo chamado: A logical calculus of the ideas immanent in nervous activit, que é uma simplificação do que se sabia até então sobre o neurônio biológico, este modelo foi chamado de MCP. 5.1.1 Modelo MCP A representação matemática do modelo MCP contém \\(n\\) terminais \\(x_1, x_2, ..., x_n\\) (representando os dendritos) e um \\(y\\) (axônio) representando o terminal de saída. Para simular o comportamento das sinapses, os terminais de entrada do neurônio tem pesos \\(W_1, W_2, ..., W_n\\) associados cujo os valores podem ser positivos ou negativos. No modelo MCP a ativação de um neurônio é obtida através de uma função de ativação, que ativa ou não a saída, dependendo do valor da soma ponderada das suas entradas (Braga, 2007). O modelo terá a saída ativada quando \\[ \\begin{eqnarray} \\sum_{i=1}^{n}x_i W_i \\geq \\theta, \\end{eqnarray} \\] em que \\(n\\) é o número de entradas, \\(W_i\\) é o peso associado a entrada \\(x_i\\), e \\(\\theta\\) (threshold) é o limiar do neurônio. A figura 2.2 ilustra o processo do modelo MCP de um neurônio artificial 5.1.2 Funções de ativação Após McCulloch e Pitts terem proposto um modelo, derivou-se outros modelos que permitem a saída não apenas zero ou um, mas diferentes valores para y. Esse processamento se dá pelo que chamamos função de ativação (Braga, 2007). Será exemplificado três tipos de função de ativação : Função Linear, definida como: \\[ \\begin{eqnarray} \\hat{y} = \\alpha x, \\end{eqnarray} \\] no qual \\(\\hat{y}\\) é a saída do modelo, \\(x\\) os valores de entrada e \\(\\alpha\\) uma constante de linearidade. No qual o máximo que y pode assumir é \\(\\gamma\\) e mínimo -\\(\\gamma\\) Função Degrau, definida como \\[ \\hat{y} = \\begin{cases} -\\gamma \\hbox{, se x $\\leq$ - $\\gamma$} \\\\ \\gamma,\\hbox{ se x $\\geq$ $\\gamma$} \\end{cases} \\] Função Logística, definida como \\[ \\begin{eqnarray} \\hat{y} = \\frac{1}{1 + \\epsilon^{x/T}}, \\end{eqnarray} \\] onde \\(T\\) determina a suavidade da curva, que será utilizada como função de ligação no modelo proposto. 5.1.3 Erro e Atualização dos pesos no MCP Quando se estima um parâmetro, é esperado que o mesmo apresente algum desvio em torno do valor real. Com o objetivo de minimizar a diferença entre eles, o algorítimo de aprendizado do perceptron dispõe de regras que permite a adaptação dos seus pesos de forma que a rede execute uma determinada tarefa de classificação. O algoritmo consiste em atualizar os pesos de modo que estejam mais próximos da solução desejada que os anteriores. A atualização do peso é dada por \\[ \\begin{eqnarray} W(t+1) = W(t) + \\Delta W, \\end{eqnarray} \\] em que \\(W(t)\\) é o peso no instante \\(t\\) e \\(\\Delta W= \\eta x\\) o incremento associado ao peso, sendo \\(\\eta\\) a taxa de aprendizado. Vamos denotar \\(x\\) o vetor de entrada, \\(y\\) o vetor da saída desejada, e {x,\\(y\\)} o nodo arbitrário da rede. A atual saída da rede será chamada de \\(\\hat{y}\\), ou seja, uma estimação do valores ideais, mas com um determinado erro \\(\\epsilon\\). Dessa forma podemos denotar o erro como \\[ \\begin{eqnarray} \\epsilon = y - \\hat{y}. \\end{eqnarray} \\] Para o caso perceptron, \\(y\\) \\(\\in\\) \\(\\{0,1\\}\\) e \\(\\hat{y}\\) \\(\\in\\) {0,1}. Dessa forma existem quatro possíveis soluções para \\(\\epsilon\\). Se \\(\\epsilon = 0\\) então encontramos valores estimados ideais. Caso contrario \\(\\epsilon\\) pode assumir valores \\(\\{-1,1\\}\\). Se \\(\\epsilon =-1\\) implica que \\(y = 0\\) e $ = 1$, mas se \\(\\epsilon =1\\) então \\(y = 1\\) e \\(\\hat{y} = 0\\). Com o valor de \\(\\epsilon\\) podemos ter uma ideia quanto a direção com que os valores estimados estão distante dos valores ideias (Braga, 2007). Feito isso, sabemos que \\(\\Delta W\\)=\\(\\eta x\\) e \\(w(t + 1) = w(t) + \\epsilon \\eta x\\). Quando \\(\\epsilon = 1\\) a equação de atualização dos pesos possui a forma \\(W(t+1)= w(t) + \\eta x\\). Mas para \\(\\epsilon = -1\\) temos a equação \\(W(t+1)= w(t) - \\eta x\\). 5.1.4 Multilayer Perceptron (MLP) O MLP é uma rede neural semelhante ao perceptron, mas com duas ou mais camadas de neurônios, sendo essas camadas ligadas entre si por sinapses com pesos. O aprendizado deste tipo de rede é feito geralmente pelo algoritmo back-propagation (retro-propagação do erro), porém existem outros algoritmos para serem usados com a mesma finalidade. A figura 2.3 ilustra a estrutura do modelo MLP quanto as entradas (inputs), camadas e a resposta obtida De forma simplificada a estrutura matemática do modelo é semelhante a do modelo perceptron sendo apenas adicionadas mais camadas de neurônios. A ativação via função de ativação é geralmente feita com a função sigmoidal comumente chamada de função logística (Braga, 2007). 5.1.5 Algoritmo back-propagation O algoritmo back-propagation é um algoritmo supervisionado que utiliza pares como entrada e saída desejada para por meio de um mecanismo de correção de erros atualizar os pesos. O treinamento tem duas fases chamadas de forward e backward, sendo que o algoritmo percorre a rede em um sentido para cada fase. A fase farward define a saida de rede para uma dada entrada inserida no modelo e a fase backward utiliza a saida fornecida e a saida desejada para atualizar os pesos de sua conexões visando a diminuição do erro (Haykin, 2007). A figura 2.4 ilustra o processo do algoritmo O algoritmo back-propagation é baseado na regra delta, sendo chamado também de regra delta generalizada. Os ajustes dos pesos são feitos utilizando-se o método gradiente. A função de custo a ser minimizada é dada pela soma dos erros quadráticos \\[ \\begin{eqnarray} E = \\frac{1}{2}\\sum_{p}\\sum_{i=1}^{k}(y_{i}^{p}-\\hat{y}_{i}^{p}), \\end{eqnarray} \\] em que \\(p\\) é o número de padrões, \\(k\\) é o número de unidades de saída, \\(y_{i}\\) é a i-ésima saída desejada e \\(\\hat{y}_{i}\\) é a i-ésima saída da rede. 5.2 Análise de Componentes Principais A análise de componentes principais é uma técnica estatística multivariada que consiste na diminuição da quantidade de variáveis em componentes que a partir delas podemos explicar uma determinada proporção da variabilidade total dos dados utilizando apenas as componentes que são essencialmente em menor número que as variáveis. Seja \\(a_{ik}\\) autovetor correspondente a i-ésima componente principal e a uma variável \\(k\\). Logo, a n-ésima componente principal para uma observação da amostra \\(j\\), denominada por \\(c_{n}^{j}\\) definida por: \\[c_{n}^{j}=\\sum_{l=1}^{k}a_{nl}x_{l}^{j},\\] onde, \\(x_{l}^{j}\\) representa o valor da l-ésima variável para uma observação j da amostra. Para o estudo em questão, os bancos de dados observados foram os do ano de 2014 e 2006 fornecidos pela cliente. Podemos representar matricialmente cada um dos bancos de dados na seguinte forma: \\[ \\textbf{X}= \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; \\dots &amp; x_{1n} \\\\ x_{21} &amp; x_{22} &amp; x_{23} &amp; \\dots &amp; x_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{d1} &amp; x_{d2} &amp; x_{d3} &amp; \\dots &amp; x_{dn} \\end{bmatrix} \\] onde, \\(n\\) é o número de variáveis associadas ao estudo sendo as colunas do nosso banco de dados, e \\(d\\) é a quantidade de observações em cada variável sendo o número de linhas do banco de dados. Com base na matriz de correlação fornecida podemos obter o vetor de autovalores e seus respectivos autovetores. O vetor de autovalores pode ser representado da seguinte forma: \\[ \\boldsymbol{\\lambda}= \\begin{bmatrix} \\lambda_{1;1} \\\\ \\lambda_{2;1}\\\\ \\vdots \\\\ \\lambda_{p;1} \\end{bmatrix} \\] Cada autovalor possui um autovetor que é associado a quantidade de variáveis observadas originalmente (número de colunas). Dessa forma, temos \\(p\\) (quantidade de componentes) autovetores cada um com \\(n\\) (quantidade de variáveis) observações, ou seja, \\(n\\) linhas. Os autovetores podem ser representados matricialmente da seguinte forma: \\[ \\textbf{A}_1= \\begin{bmatrix} a_{1;1} \\\\ a_{2;1}\\\\ \\vdots \\\\ a_{n;1} \\end{bmatrix} \\] \\[ \\textbf{A}_2= \\begin{bmatrix} a_{1;1} \\\\ a_{2;1}\\\\ \\vdots \\\\ a_{n;1} \\end{bmatrix} \\] \\[ \\textbf{A}_{p}= \\begin{bmatrix} a_{1;1} \\\\ a_{2;1}\\\\ \\vdots \\\\ a_{n;1} \\end{bmatrix} .\\] Conforme definido anteriormente, o cálculo da n-ésima componente principal para uma observação da amostra j, denominada por \\(c_{n}^{j}\\) é definida por: \\[c_{n}^{j}=\\sum_{l=1}^{17}a_{nl}x_{l}^{j}.\\] Para facilitar o entendimento, a matriz com todas as componentes principais pode ser representada da seguinte forma: \\[ \\textbf{C}= \\begin{bmatrix} C_{1}^{1} &amp; C_{2}^{1} &amp; \\dots &amp; C_{p}^{1} \\\\ C_{1}^{2} &amp; C_{2}^{2} &amp; \\dots &amp; C_{p}^{2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ C_{1}^{d} &amp; C_{2}^{d} &amp; \\dots &amp; C_{p}^{d}\\\\ \\end{bmatrix} .\\] O índice de capital social para cada observação \\(j\\), denominado por \\(ICS_j\\), é dado por: \\[ICS_j=\\frac{1}{\\sum_{k=1}^{p}\\lambda_k}\\sum_{k=1}^{p}\\lambda_k c_{k}^{j},\\] em que \\(\\lambda_k\\) representa o autovalor associado à k-ésima componente principal, sendo \\(p\\) o número de componentes principais utilizadas na construção do índice (quantidade definida a partir da variabilidade miníma explicada aceitável). O vetor com todos os índices são dados na forma: \\[ \\textbf{I}= \\begin{bmatrix} ICS_{1;1} \\\\ ICS_{2;1}\\\\ \\vdots \\\\ ICS_{j;1}\\\\ \\vdots\\\\ ICS_{d;1} \\end{bmatrix} ,\\] sendo \\(d\\) o número de observações. "],["amostragem.html", "Capítulo 6 Amostragem 6.1 Amostragem Aleatória Simples sem reposição - AASs", " Capítulo 6 Amostragem 6.1 Amostragem Aleatória Simples sem reposição - AASs Para a confecção desse trabalho foi utilizada amostragem aleatória simples sem reposição, uma vez que o espaço amostral era composto por idosos que são acompanhados pelo posto de saúde, ou seja, o mesmo idoso é acompanhado por apenas um posto de saúde. Levou-se em consideração que cada idoso poderia ou não ser selecionado, perante isso, utiliza-se na Estatística a distribuição de Bernoulli. Sendo \\(x_i\\) o idoso observado, então: \\[x_i = \\begin{cases} 1,\\text{ se o idoso é selecionado para compor a amostra.} \\\\ 0,\\text{ se o idoso não é selecionado para compor amostra.} \\\\ \\end{cases} \\] De acordo com a distribuição assumida, pode-se calcular a variância dos dados por: \\[\\sigma^2 = p(1 - p),\\] em que: \\(\\sigma^2 =\\) Variabilidade dos indivíduos na nossa população; \\(p \\ \\ =\\) Proporção de membros da população em estudo que apresenta a característica. Como não teve-se acesso a nenhum dado anterior que pudesse estimar a variabilidade dos dados estudados, tais como pesquisas anteriores, optou-se por usar a maior variabilidade possível, denotada na estatística como variabilidade conservadora, no qual \\(p\\) assume o valor que maior estima a variância, no caso, 0,5. Daí, segue-se que, \\[S^2 = p(1 - p) = 0,5(1 - 0,5) = 0,25 = \\frac{1}{4},\\] onde \\(S^2\\) é a variância estimada. Diante tudo isso, calcula-se o tamanho amostral, denotado por \\(n\\), através da seguinte expressão: \\[n = \\frac{1}{\\frac{D}{S^2}+\\frac{1}{N}},\\] em que: \\(D \\ = \\frac{E^2}{Z_\\alpha};\\) \\(E \\ =\\) Erro máximo permitido; \\(Z_\\alpha =\\) Quantil de ordem \\(\\alpha\\) da distribuição normal padrão; \\(N \\ =\\) Tamanho populacional; \\(S^2 =\\) Variância populacional estimada. O Erro máximo permitido ou margem de erro é a diferença entre o valor estimado pela pesquisa e o verdadeiro valor. Por exemplo, se retirarmos a média \\(\\mu\\) de uma amostra com uma margem de erro de 10% esperamos que a média da população esteja em um intervalo de \\(\\mu - 10\\%\\) e \\(\\mu + 10\\%\\). Para estimarmos uma determinada característica de interesse com dada precisão, é importante que estabeleçamos um nível de confiança para nossa pesquisa, nível esse que denotaremos por \\((1 - \\alpha)\\), sendo a probabilidade de que o erro amostral efetivo seja menor do que o erro amostral admitido pela pesquisa (máximo permitido). É importante salientar que um alto nível de confiança pode trazer problemas atrelados a ele, por exemplo, uma necessidade de um maior tamanho amostral. Margem de erro, Coeficiente de confiança e tamanho da amostra sempre caminham lado a lado. Modificar qualquer um dos 3 parâmetros, alterará os restantes. Encontra-se algumas mudanças: Reduzir a margem de erro obriga a aumentar o tamanho da amostra; Diminuir o Coeficiente de confiança obriga a aumentar o tamanho da amostra; Se eu aumentar o tamanho da minha amostra, posso reduzir a margem de erro ou incrementar o nível de confiança. Existe casos onde o tamanho amostral fica inacessível para o pesquisador devido aos custos gerados, para isto existe um fator de correção para diminuir o tamanho amostral (n), quando \\(\\frac{n}{N} \\geq 0,05\\). Segue o novo tamanho amostral: \\[n_{new} = \\frac{n}{1 + \\left(\\frac{n - 1}{N}\\right)}\\] em que: \\(n_{new} =\\) Novo tamanho amostral com fator de correção; \\(\\ n \\ \\ =\\) Tamanho amostral sem o fator de correção; \\(\\ N \\ =\\) Tamanho da população alvo. "],["gráficos.html", "Capítulo 7 Gráficos 7.1 Barras 7.2 Circular (Pizza) 7.3 Boxplot 7.4 Linhas", " Capítulo 7 Gráficos 7.1 Barras Este é um gráfico que utiliza barras retangulares e o seu comprimento representa os dados proporcionalmente a escala do eixo que estão os dados. Estas Barras podem estar na vertical ou na Horizontal.Além disso, é um gráfico muito utilizado para representar valores discretos.Neles conseguimos compreender a quantidade de cada item pelo tamanho da barra retangular. Histograma é um gráfico de barras que demonstra a distribuição de dados e frequências. A base de cada uma de suas barras representa uma classe e a altura a frequência em que a classe ocorre. 7.2 Circular (Pizza) Este é um gráfico que cada fatia representa uma categoria de dados que compõe o todo. Juntas, as fatias representam 100%. O tamanho de cada fatia é relativo à sua porção do todo. Se há muitas categorias de dados apresentadas, a leitura do gráfico de pizza pode ficar bem prejudicada. O intuito do gráfico de pizza é que todas as informações fiquem claras à primeira vista. Um gráfico de rosca é um tipo de gráfico de pizza, mas em vez de apresentar os dados em um formato de círculo sólido, apresenta as informações em um formato de rosca. Eles têm basicamente a mesma função e podem ser usados alternadamente, de acordo do aspecto visual que você estiver buscando. 7.3 Boxplot Este é um gráfico que utiliza um diagrama de caixa é uma ferramenta gráfica que permite visualizar a distribuição e valores discrepantes (outliers) dos dados.Além disso, o boxplot também é uma disposição gráfica comparativa.As medidas de estatísticas descritivas como o mínimo, máximo, primeiro quartil, segundo quartil ou mediana e o terceiro quartil formam o boxplot. 7.3.1 Boxplot + Violino O violino são as curvas no entorno do gráfico boxplot, e representa a função densidade de probabilidade estimada via kernel (basicamente, as curvas mais largas representam maior densidade de pontos, ou seja, existe uma maior frequência de pontos). Esse tipo de representação pode ser útil para alguns conjuntos de dados, pois pelo fato do boxplot resumir os dados em 5 medidas descritivas, pode haver perda de informação. Já o violino resume os dados em uma função densidade, caracterizando melhor o conjunto de dados. 7.4 Linhas Este é um gráfico de linhas continuas para a visualização de dados em linhas do tempo, serve para ver a tendencia e a sazonalidade do que esta ocorrendo com os dados,geralmente o eixo Y são onde fica os dias,meses ou anos e o eixo X é onde fica uma escala discreta ou continua em relação ao tipo de dado. "],["referências.html", "Capítulo 8 Referências", " Capítulo 8 Referências Agresti, A. (2002). Categorical data analysis. Second edition. New York: Wiley. Pages 91101 Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd ed. New York: John Wiley &amp; Sons. Page 38 Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd ed. New York: John Wiley &amp; Sons. Page 29 Bonferroni, C. E., Teoria statistica delle classi e calcolo delle probabilità, Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze 193 Bussab,W. O.; Morettin, P. A.Estatística Básica - 7 Edição, Atual Editora, 1987. R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. Lance, C.E., &amp; Vandenberg, R.J. (Eds.). (2014). More Statistical and Methodological Myths and Urban Legends: Doctrine, Verity and Fable in Organizational and Social Sciences (1st ed.). Routledge. https://doi.org/10.4324/9780203775851 Associação entre variáveis.Disponível em: http://sweet.ua.pt/andreia.hall/TEA/Capcorrel.pdf. Acesso:10 de janeiro 2018. Bussab,W. O.; Morettin, P. A.Estatística Básica - 7 Edição, Atual Editora, 1987. Bolfarine, H.;Sandoval, M. C. Introdução à Inferência Estatística} - 2 Edição, Editora SBM, 2010. Teste de hipótese. Disponível em:http://www.ufscar.br/jcfogo/Estat_2/arquivos/Teste_Hipotese.pdf. Acesso:12 de janeiro 2018. Viali,L.Apostila Testes Hipóteses Não Paramétricos.Porto Alegre, 2008. HANIFAN, Lyda J. The rural school community center. The Annals of the American Academy of Political and Social Science, v. 67, n. 1, p. 130-138, 1916. JOHNSON, R A. WICHERN, D. W. Applied Multivariate Statistical Analysis. Vol 5. No. 8. Upper Saddle River, NJ: Prentice hall, 2002. DE PÁDUA BRAGA, Antônio; DE LEON FERREIRA, André Carlos Ponce; LUDERMIR, Teresa Bernarda. Redes neurais artificiais: teoria e aplicações. Rio de Janeiro, Brazil:: LTC Editora, 2007. HAYKIN, Simon. Redes neurais: princípios e prática. Bookman Editora, 2007.. MORETTIN, Pedro Alberto; BUSSAB, WILTON OLIVEIRA. Estatística básica. Editora Saraiva, 2017. PAULA, Gilberto Alvarenga. Modelos de regressão: com apoio computacional. São Paulo: IME-USP, 2004. Scikit-Learn, Multilayer Perceptron. Disponível em:https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html. "]]
